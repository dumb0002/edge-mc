{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#mutlicluster-configuration-management-for-edge-multi-cloud-and-hybrid-cloud","title":"Mutlicluster Configuration Management for Edge, Multi-Cloud, and Hybrid Cloud","text":""},{"location":"#what-is-kubestellar","title":"What is KubeStellar?","text":"<p>We are an opensource community focused on creating a flexible solution for challenges associated with mutlicluster configuration management for edge, multi-cloud, and hybrid cloud:</p> <ul> <li> <p>Disconnected Operation: clusters don't always have connectivity, we get it</p> </li> <li> <p>Large Scale Deployments: the world is a big place and clusters can exist anywhere. Farms, Cruise Ships, Oil Rigs, even Space</p> </li> <li> <p>Small Locations: clusters can come in all shapes and sizes. MicroShift, K3s, and Kind can enable help small take part in a Kubernetes environment</p> </li> <li> <p>Different Types of Clouds: Edge, sovereign, regulated, high-performance, on-prem - we got you covered</p> </li> </ul>"},{"location":"#so-what-are-we-working-on-to-solve-these-challenges","title":"So, what are we working on to solve these challenges?","text":"<ul> <li>Desired placement expression\u200b: Need a way for one center object to express large number of desired copies\u200b</li> <li>Scheduling/syncing interface\u200b: Need something that scales to large number of destinations\u200b</li> <li>Rollout control\u200b: Client needs programmatic control of rollout, possibly including domain-specific logic\u200b</li> <li>Customization: Need a way for one pattern in the center to express how to customize for all the desired destinations\u200b</li> <li>Status from many destinations\u200b: Center clients may need a way to access status from individual edge copies</li> <li>Status summarization\u200b: Client needs a way to specify how statuses from edge copies are processed/reduced along the way from edge to center\u200b.</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Do you have 3 minutes to try our solution?  Head on over to our Quickstart Guide</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing guide.</p>"},{"location":"#getting-in-touch","title":"Getting in touch","text":"<p>Subscribe to the community calendar{ .md-button .md-button--primary }</p> <p>There are several ways to communicate with us:</p> <ul> <li>The <code>#kubestellar-dev</code> channel in the Kubernetes Slack workspace</li> <li>Our mailing lists:<ul> <li>kubestellar-dev for development discussions</li> <li>kubestellar-users for discussions among users and potential users</li> </ul> </li> <li>Subscribe to the community calendar or download a meeting series invite for community meetings and events<ul> <li>The kubestellar-dev mailing list is subscribed to this calendar</li> </ul> </li> <li>See recordings of past KubeStellar community meetings on YouTube</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Browse the shared Google Drive to share design docs, notes, etc.<ul> <li>Members of the kubestellar-dev mailing list can view this drive</li> </ul> </li> <li>Read our documentation</li> <li>Follow us on:<ul> <li>LinkedIn - #kubestellar</li> <li>Medium - kubestellar.medium.com</li> </ul> </li> </ul>"},{"location":"#contributors","title":"\u2764\ufe0f Contributors","text":"<p>Thanks go to these wonderful people:</p> Jun Duan\ud83d\udc40 Braulio Dumba\ud83d\udc40 Mike Spreitzer\ud83d\udc40 Paolo Dettori\ud83d\udc40 Andy Anderson\ud83d\udc40 Franco Stellari\ud83d\udc40 Ezra Silvera\ud83d\udc40 Bob Filepp\ud83d\udc40 Alexei Karve\ud83d\udc40 Maria Camila Ruiz Cardenas\ud83d\udc40"},{"location":"Coding%20Milestones/PoC2023q1/coding-milestone-invite-q1/","title":"Invitation","text":"<p>Dear Contributors,</p> <p>We are excited to invite you to join the first KubeStellar opensource community coding sprint. We will be focus on several key projects that are critical to the development of state-based edge solutions. Our collective work will be showcased to the opensource community on Thursday, April 27th.</p> <p>This coding sprint will provide a great opportunity for you to showcase your skills, learn new techniques, and collaborate with other experienced engineers in the KubeStellar community. We believe that your contributions will be invaluable in helping us achieve our goals and making a lasting impact in the field of state-based edge technology.</p> <p>The coding sprint will be dedicated to completing the following workload management elements:</p> <ul> <li>Implementing an edge scheduler and placement translator, including customization options,</li> <li>Incorporating existing customization API into the KubeStellar repo,</li> <li>Investigating implementation of a status summarizer, starting with basic implicit status, and later adding programmed summarization,</li> <li>Updating summarization API and integrating it into the KubeStellar repo,</li> <li>Defining the API for identifying associated objects and its interaction with summarization, and implementing these,</li> <li>Streamlining the creation of workload management workspaces,</li> <li>Examining the use of Postgresql through Kine instead of etcd for scalability,</li> <li>Revising the milestone outline with regards to defining bootstrapping and support for cluster-scoped resources.</li> </ul> <p>In addition to workload management, we will also be working on inventory management for the demo, as well as designing various demo scenarios, including a baseline demo with kubectl, demos with ArgoCD, FluxCD, and the European Space Agency (ESA). To support the engineers and demonstrations we will also need to automate the process of creating infrastructure, deploying demo pieces and instrumentation, bootstrapping, running scenarios, and collecting data.</p> <p>If you are interested in joining us for this exciting coding sprint, please check out our 'good first issue' list, or slack me @Andy Anderson so I can connect you with others in your area of interest.  There is a place for every skillset to contribute. Not quite sure?  You can join our bi-weekly community meetings to watch our progress.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/","title":"2023q1 PoC commands","text":"<p>This PoC includes two sorts of commands for users to use.  Most are executables delivered in the <code>bin</code> directory.  The other sort of command for users is a <code>bash</code> script that is designed to be fetched from github and fed directly into <code>bash</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#executables","title":"Executables","text":"<p>The command lines exhibited below presume that you have added the <code>bin</code> directory to your <code>$PATH</code>.  Alternatively: these executables can be invoked directly using any pathname (not in your <code>$PATH</code>).</p> <p>NOTE: all of the kubectl plugin usages described here certainly or potentially change the setting of which kcp workspace is \"current\" in your chosen kubeconfig file; for this reason, they are not suitable for executing concurrently with anything that depends on that setting in that file.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#platform-control","title":"Platform control","text":"<p>The <code>kubestellar</code> command has three subcommands, one to finish setup and two for process control.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubestellar [flags] subcommand [flags]\n</code></pre> <p>This command accepts the following command line flags, which can appear before and/or after the subcommand.  The <code>--log-folder</code> flag is only used in the <code>start</code> subcommand.</p> <ul> <li><code>-V</code> or <code>--verbose</code>: calls for more verbose output.  This is a   binary choice, not a matter of degree.</li> <li><code>--log-folder $pathname</code>: says where to put the logs from the   controllers.  Will be <code>mkdir -p</code> if absent.  Defaults to   <code>${PWD}/kubestellar-logs</code>.</li> <li><code>-h</code> or <code>--help</code>: print a brief usage message and terminate.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-init","title":"Kubestellar init","text":"<p>This subcommand is used after installation to finish setup.</p> <p>This subcommand ensures that the edge service provider workspace (ESPW) exists and has the required contents.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-start","title":"KubeStellar start","text":"<p>This subcommand is used after installation or process stops.</p> <p>This subcommand stops any running kubesteallar controllers and then starts them all.  It also does the same thing as <code>kubestellar init</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-stop","title":"KubeStellar stop","text":"<p>This subcommand undoes the primary function of <code>kubestellar start</code>, stopping any running KubeStellar controllers.  It does not tear down the ESPW.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-release","title":"KubeStellar-release","text":"<p>This command just echoes the semantic version of the release used.  This command is only available in archives built for a release.  Following is an example usage.</p> <pre><code>$ kubestellar-release\nv0.2.3-preview\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-version","title":"Kubestellar-version","text":"<p>This executable prints information about itself captured at build time.  If built by <code>make</code> then this is information conveyed by the Makefile; otherwise it is the Kubernetes defaults.</p> <p>It will either print one requested property or a JSON object containing many.</p> <pre><code>$ kubestellar-version help\nInvalid component requested: \"help\"\nUsage: kubestellar-version [buildDate|gitCommit|gitTreeState|platform]\n$ kubestellar-version buildDate\n2023-05-19T02:54:01Z\n$ kubestellar-version gitCommit\n1747254b\n$ kubestellar-version          {\"major\":\"1\",\"minor\":\"24\",\"gitVersion\":\"v1.24.3+kcp-v0.2.1-20-g1747254b880cb7\",\"gitCommit\":\"1747254b\",\"gitTreeState\":\"dirty\",\"buildDate\":\"2023-05-19T02:54:01Z\",\"goVersion\":\"go1.19.9\",\"compiler\":\"gc\",\"platform\":\"darwin/amd64\"}\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#creating-synctargetlocation-pairs","title":"Creating SyncTarget/Location pairs","text":"<p>In this PoC, the interface between infrastructure and workload management is inventory API objects.  Specifically, for each edge cluster there is a unique pair of SyncTarget and Location objects in a so-called inventory management workspace.  The following command helps with making that pair of objects.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubectl kubestellar ensure location flag... objname labelname=labelvalue...\n</code></pre> <p>Here <code>objname</code> is the name for the SyncTarget object and also the name for the Location object.  This commad ensures that these objects exist and have at least the given labels.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>The acceptable flags include all those of <code>kubectl</code> except for <code>--context</code>.  This command also accepts the following flags.</p> <ul> <li><code>--imw workspace_path</code>: specifies which workspace to use as the   inventory management workspace.  The default value is the current   workspace.</li> </ul> <p>The current workspaces does not matter if the IMW is explicitly specified.  Upon completion, the current workspace will be your chosen IMW.</p> <p>This command does not depend on the action of any of the edge-mc (KubeStellar) controllers.</p> <p>An example usage follows.</p> <pre><code>$ kubectl kubestellar ensure location --imw root:imw-1 demo1 foo=bar the-word=the-bird\nCurrent workspace is \"root:imw-1\".\nsynctarget.workload.kcp.io/demo1 created\nlocation.scheduling.kcp.io/demo1 created\nsynctarget.workload.kcp.io/demo1 labeled\nlocation.scheduling.kcp.io/demo1 labeled\nsynctarget.workload.kcp.io/demo1 labeled\nlocation.scheduling.kcp.io/demo1 labeled\n</code></pre> <p>The above example shows using this script to create a SyncTarget and a Location named <code>demo1</code> with labels <code>foo=bar</code> and <code>the-word=the-bird</code>. This was equivalent to the following commands.</p> <pre><code>kubectl ws root:imw-1\nkubectl create -f -&lt;&lt;EOF\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\n  name: demo1\n  labels:\n    id: demo1\n    foo: bar\n    the-word: the-bird\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\n  name: demo1\n  labels:\n    foo: bar\n    the-word: the-bird\nspec:\n  resource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\n  instanceSelector:\n    matchLabels: {\"id\":\"demo1\"}\nEOF\n</code></pre> <p>This command operates in idempotent style, making whatever changes (if any) are needed to move from the current state to the desired state. Current limitation: it does not cast a skeptical eye on the spec of a pre-existing Location.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#removing-synctargetlocation-pairs","title":"Removing SyncTarget/Location pairs","text":"<p>The following script undoes whatever remains from a corresponding usage of <code>kubectl kubestellar ensure location</code>.  It has all the same command line syntax and semantics except that the <code>labelname=labelvalue</code> pairs do not appear.</p> <p>This command does not depend on the action of any of the edge-mc (KubeStellar) controllers.</p> <p>The following sesssion demonstrates usage, including idemptotency.</p> <pre><code>$ kubectl ws root:imw-1\nCurrent workspace is \"root:imw-1\".\n$ kubectl kubestellar remove location demo1\nsynctarget.workload.kcp.io \"demo1\" deleted\nlocation.scheduling.kcp.io \"demo1\" deleted\n$ kubectl kubestellar remove location demo1\n\n$ \n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#syncer-preparation-and-installation","title":"Syncer preparation and installation","text":"<p>The syncer runs in each edge cluster and also talks to the corresponding mailbox workspace.  In order for it to be able to do that, there is some work to do in the mailbox workspace to create a ServiceAccount for the syncer to authenticate as and create RBAC objects to give the syncer the privileges that it needs.  The following script does those things and also outputs YAML to be used to install the syncer in the edge cluster.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubectl kubestellar prep-for-syncer flag... synctarget_name\n</code></pre> <p>Here <code>synctarget_name</code> is the name of the <code>SyncTarget</code> object, in the relevant IMW, corresponding to the relevant edge cluster.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>The acceptable flags include all those of <code>kubectl</code> except for <code>--context</code>.  This command also accepts the following flags.</p> <ul> <li><code>--imw workspace_path</code>: specifies which workspace holds the relevant   SyncTarget object.  The default value is the current workspace.</li> <li><code>--espw workspace_path</code>: specifies where to find the edge service   provider workspace.  The default is the standard location,   <code>root:espw</code>.</li> <li><code>--syncer-image image_ref</code>: specifies the container image that runs   the syncer.  The default is <code>quay.io/kubestellar/syncer:v0.2.1</code>.</li> <li><code>-o output_pathname</code>: specifies where to write the YAML definitions   of the API objects to create in the edge cluster in order to deploy   the syncer there.  The default is <code>synctarget_name +   \"-syncer.yaml\"</code>.</li> </ul> <p>The current workspaces does not matter if the IMW is explicitly specified.  Upon completion, the current workspace will be what it was when the command started.</p> <p>This command will only succeed if the mailbox controller has created and conditioned the mailbox workspace for the given SyncTarget.  This command will wait for 10 to 70 seconds for that to happen.</p> <p>An example usage follows.</p> <pre><code>$ kubectl kubestellar prep-for-syncer --imw root:imw-1 demo1\nCurrent workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\"\nCurrent workspace is \"root:espw:4yqm57kx0m6mn76c-mb-406c54d1-64ce-4fdc-99b3-cef9c4fc5010\" (type root:universal).\nCreating service account \"kcp-edge-syncer-demo1-28at01r3\"\nCreating cluster role \"kcp-edge-syncer-demo1-28at01r3\" to give service account \"kcp-edge-syncer-demo1-28at01r3\"\n 1. write and sync access to the synctarget \"kcp-edge-syncer-demo1-28at01r3\"\n 2. write access to apiresourceimports.\nCreating or updating cluster role binding \"kcp-edge-syncer-demo1-28at01r3\" to bind service account \"kcp-edge-syncer-demo1-28at01r3\" to cluster role \"kcp-edge-syncer-demo1-28at01r3\".\nWrote physical cluster manifest to demo1-syncer.yaml for namespace \"kcp-edge-syncer-demo1-28at01r3\". Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"demo1-syncer.yaml\"\nto apply it. Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kcp-edge-syncer-demo1-28at01r3\" kcp-edge-syncer-demo1-28at01r3\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre> <p>Once that script has run, the YAML for the objects to create in the edge cluster is in your chosen output file.  The default for the output file is the name of the SyncTarget object with \"-syncer.yaml\" appended.</p> <p>Create those objects with a command like the following; adjust as needed to configure <code>kubectl</code> to modify the edge cluster and read your chosen output file.</p> <pre><code>KUBECONFIG=$demo1_kubeconfig kubectl apply -f demo1-syncer.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#edge-cluster-on-boarding","title":"Edge cluster on-boarding","text":"<p>The following command is a combination of <code>kubectl kubestellar ensure-location</code> and <code>kubectl kubestellar prep-for-syncer</code>, and takes the union of their command line flags and arguments.  Upon completion, the kcp current workspace will be what it was at the start.</p> <p>An example usage follows.</p> <pre><code>$ kubectl kubestellar prep-for-cluster --imw root:imw-1 demo2 key1=val1\nCurrent workspace is \"root:imw-1\".\nsynctarget.workload.kcp.io/demo2 created\nlocation.scheduling.kcp.io/demo2 created\nsynctarget.workload.kcp.io/demo2 labeled\nlocation.scheduling.kcp.io/demo2 labeled\nCurrent workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1cpf1cd4ydy13vo1-mb-3c354acd-ed86-45bb-a60d-cee8e59973f7\" (type root:universal).\nCreating service account \"kcp-edge-syncer-demo2-15nq4e94\"\nCreating cluster role \"kcp-edge-syncer-demo2-15nq4e94\" to give service account \"kcp-edge-syncer-demo2-15nq4e94\"\n 1. write and sync access to the synctarget \"kcp-edge-syncer-demo2-15nq4e94\"\n 2. write access to apiresourceimports.\nCreating or updating cluster role binding \"kcp-edge-syncer-demo2-15nq4e94\" to bind service account \"kcp-edge-syncer-demo2-15nq4e94\" to cluster role \"kcp-edge-syncer-demo2-15nq4e94\".\nWrote physical cluster manifest to demo2-syncer.yaml for namespace \"kcp-edge-syncer-demo2-15nq4e94\". Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"demo2-syncer.yaml\"\nto apply it. Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kcp-edge-syncer-demo2-15nq4e94\" kcp-edge-syncer-demo2-15nq4e94\nto verify the syncer pod is running.\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#creating-a-workload-management-workspace","title":"Creating a Workload Management Workspace","text":"<p>Such a workspace needs not only to be created but also populated with an <code>APIBinding</code> to the edge API and, if desired, an <code>APIBinding</code> to the Kubernetes API for management of containerized workloads.</p> <p>NOTE: currently, only a subset of the Kubernetes containerized workload management API is supported.  In particular, only the following object kinds are supported: <code>Deployment</code>, <code>Pod</code>, <code>Service</code>, <code>Ingress</code>.  To be clear: this is in addition to the generic object kinds that are supported; illustrative examples include RBAC objects, <code>CustomResourceDefinition</code>, and <code>ConfigMap</code>.  For a full description, see the categorization in the design.</p> <p>The usage synopsis for this command is as follows.</p> <pre><code>kubectl ws parent_pathname; kubectl kubestellar ensure wmw flag... wm_workspace_name\n</code></pre> <p>Here <code>parent_pathname</code> is the workspace pathame of the parent of the WMW, and <code>wm_workspace_name</code> is the name (not pathname, just a bare one-segment name) of the WMW to ensure.  Thus, the pathname of the WMW will be <code>parent_pathname:wm_workspace_name</code>.</p> <p>Upon completion, the WMW will be the current workspace.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>The acceptable flags include all those of <code>kubectl</code> except for <code>--context</code>.  This command also accepts the following flags.</p> <ul> <li><code>--with-kube boolean</code>: specifies whether or not the WMW should   include an APIBinding to the Kubernetes API for management of   containeried workloads.</li> </ul> <p>This script works in idempotent style, doing whatever work remains to be done.</p> <p>The following session shows some example usages, including demonstration of idempotency and changing whether the kube APIBinding is included.</p> <pre><code>$ kubectl ws .\nCurrent workspace is \"root:my-org\".\n$ kubectl kubestellar ensure wmw example-wmw\nCurrent workspace is \"root\".\nCurrent workspace is \"root:my-org\".\nWorkspace \"example-wmw\" (type root:universal) created. Waiting for it to be ready...\nWorkspace \"example-wmw\" (type root:universal) is ready to use.\nCurrent workspace is \"root:my-org:example-wmw\" (type root:universal).\napibinding.apis.kcp.io/bind-espw created\napibinding.apis.kcp.io/bind-kube created\n$ kubectl ws ..\nCurrent workspace is \"root:my-org\".\n$ kubectl kubestellar ensure wmw example-wmw\nCurrent workspace is \"root\".\nCurrent workspace is \"root:my-org\".\nCurrent workspace is \"root:my-org:example-wmw\" (type root:universal).\n$ kubectl ws ..\nCurrent workspace is \"root:my-org\".\n$ kubectl kubestellar ensure wmw example-wmw --with-kube false\nCurrent workspace is \"root\".\nCurrent workspace is \"root:my-org\".\nCurrent workspace is \"root:my-org:example-wmw\" (type root:universal).\napibinding.apis.kcp.io \"bind-kube\" deleted\n$ \n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#removing-a-workload-management-workspace","title":"Removing a Workload Management Workspace","text":"<p>Deleting a WMW can be done by simply deleting its <code>Workspace</code> object from the parent.</p> <pre><code>$ kubectl ws .\nCurrent workspace is \"root:my-org:example-wmw\".\n$ kubectl ws ..\nCurrent workspace is \"root:my-org\".\n$ kubectl delete Workspace example-wmw\nworkspace.tenancy.kcp.io \"example-wmw\" deleted\n$ \n</code></pre> <p>Alternatively, you can use the following command line whose design completes the square here.  Invoke it when the current workspace is the parent of the workload management workspace to delete.</p> <pre><code>$ kubectl kubestellar remove wmw -h\nUsage: kubectl ws parent; kubectl kubestellar remove wmw kubectl_flag... wm_workspace_name\n$ kubectl ws root:my-org\nCurrent workspace is \"root:my-org\".\n$ kubectl kubestellar remove wmw demo1\nworkspace.tenancy.kcp.io \"demo1\" deleted\n$ kubectl ws .\nCurrent workspace is \"root:my-org\".\n$ kubectl kubestellar remove wmw demo1\n\n$ \n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#web-to-bash","title":"Web-to-bash","text":""},{"location":"Coding%20Milestones/PoC2023q1/commands/#quick-setup","title":"Quick Setup","text":"<p>This is a combination of some installation and setup steps, for use in the QuickStart.</p> <p>The script can be read directly from https://raw.githubusercontent.com/kcp-dev/edge-mc/main/bootstrap/bootstrap-kubestellar.sh and does the following things.</p> <ol> <li>Downloads and installs kcp if it is not already evident on <code>$PATH</code>    (using the script below.</li> <li>Starts a kcp server if one is not already running.</li> <li>Downloads and installs kubestellar if it is not already evident on    <code>$PATH</code> (using the script below.</li> <li><code>kubestellar start</code> if the KubeStellar controllers are not already    running or the ESPW does not (yet) exist.</li> </ol> <p>This script accepts the following command line flags; all are optional.</p> <ul> <li><code>--kubestellar-version $version</code>: specifies the release of   KubeStellar to use.  When using a specific version, include the   leading \"v\".  The default is the latest regular release, and the   value \"latest\" means the same thing.</li> <li><code>--kcp-version $version</code>: specifies the kcp release to use.  The   default is the one that works with the chosen release of   KubeStellar.</li> <li><code>--os $OS</code>: specifies the operating system to use in selecting the   executables to download and install.  Choices are <code>linux</code> and   <code>darwin</code>.  Autodetected if omitted.</li> <li><code>--arch $IAS</code>: specifies the instruction set architecture to use in   selecting the executables to download and install.  Choices are   <code>amd64</code> and <code>arm64</code>.  Autodetected if omitted.</li> <li><code>--bind-address $IPADDR</code>: directs that the kcp server (a) write that   address for itself in the kubeconfig file that it constructs and (b)   listens only at that address.  The default is to pick one of the   host's non-loopback addresses to write into the kubeconfig file and   not bind a listening address.</li> <li><code>--ensure-folder $install_parent_dir</code>: specifies the parent folder   for downloads.  Will be <code>mkdir -p</code>.  The default is the current   working directory.  The download of kcp, if any, will go in   <code>$install_parent_dir/kcp</code>.  The download of KubeStellar will go in   <code>$install_parent_dir/kubestellar</code>.</li> <li><code>-V</code> or <code>--verbose</code>: incrases the verbosity of output.  This is a   binary thing, not a matter of degree.</li> <li><code>-X</code>: makes the script <code>set -x</code> internally, for debugging.</li> <li><code>-h</code> or <code>--help</code>: print brief usage message and exit.</li> </ul> <p>Here \"install\" means only to (a) unpack the distribution archives into the relevant places under <code>$install_parent_dir</code> and (b) enhance the <code>PATH</code>, and <code>KUBECONFIG</code> in the case of kcp, environment variables in the shell running the script.  Of course, if you run the script in a subshell then those environment effects terminate with that subshell; this script also prints out messages showing how to update the environment in another shell.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#install-kcp-and-its-kubectl-plugins","title":"Install kcp and its kubectl plugins","text":"<p>This script is directly available at https://github.com/kcp-dev/edge-mc/blob/main/bootstrap/install-kubestellar.sh and does the following things.</p> <ul> <li>Fetch and install the <code>kcp</code> server executable.</li> <li>Fetch and install the kubectl plugins of kcp.</li> </ul> <p>This script accepts the following command line flags; all are optional.</p> <ul> <li><code>--version $version</code>: specifies the kcp release to use.  The default   is the latest.</li> <li><code>--OS $OS</code>: specifies the operating system to use in selecting the   executables to fetch and install.  Choices are <code>darwin</code> and <code>linux</code>.   Auto-detected if omitted.</li> <li><code>--arch $ARCH</code>: specifies the instruction set architecture to use in   selecting the executables to fetch and install.  Choices are <code>arm64</code>   and <code>amd64</code>.  Auto-detected if omitted.</li> <li><code>--ensure-folder $install_parent_dir</code>: specifies where to install   to.  This will be <code>mkdir -p</code>.  The default is <code>./kcp</code>.</li> <li><code>-V</code> or <code>--verbose</code>: incrases the verbosity of output.  This is a   binary thing, not a matter of degree.</li> <li><code>-X</code>: makes the script <code>set -x</code> internally, for debugging.</li> <li><code>-h</code> or <code>--help</code>: print brief usage message and exit.</li> </ul> <p>Here install means only to unpack the downloaded archives, creating <code>$install_parent_dir/bin</code>.  If <code>$install_parent_dir/bin</code> is not already on your <code>$PATH</code> then this script will print out a message telling you to add it.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#install-kubestellar","title":"Install KubeStellar","text":"<p>This script is direclty available at https://github.com/kcp-dev/edge-mc/blob/main/bootstrap/install-kubestellar.sh and will download and install KubeStellar.</p> <p>This script accepts the following command line arguments; all are optional.</p> <ul> <li><code>--version $version</code>: specifies the release of KubeStellar to use.   Defaults to the latest regular release.</li> <li><code>--OS $OS</code>: specifies the operating system to use in selecting the   executables to fetch and install.  Choices are <code>darwin</code> and <code>linux</code>.   Auto-detected if omitted.</li> <li><code>--arch $ARCH</code>: specifies the instruction set architecture to use in   selecting the executables to fetch and install.  Choices are <code>arm64</code>   and <code>amd64</code>.  Auto-detected if omitted.</li> <li><code>--ensure-folder $install_parent_dir</code>: specifies where to install   to.  This will be <code>mkdir -p</code>.  The default is <code>./kubestellar</code>.</li> <li><code>-V</code> or <code>--verbose</code>: incrases the verbosity of output.  This is a   binary thing, not a matter of degree.</li> <li><code>-X</code>: makes the script <code>set -x</code> internally, for debugging.</li> <li><code>-h</code> or <code>--help</code>: print brief usage message and exit.</li> </ul> <p>Here install means only to unpack the downloaded archive, creating <code>$install_parent_dir/bin</code>.  If <code>$install_parent_dir/bin</code> is not already on your <code>$PATH</code> then this script will print out a message telling you to add it.</p>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/","title":"KubeStellar Syncer","text":""},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#registering-edge-syncer-on-an-edge-cluster","title":"Registering Edge Syncer on an Edge cluster","text":"<p>Edge-syncer can be deployed on Edge cluster easily by the following steps.</p> <ol> <li>Create SyncTarget and Location<ul> <li>Mailbox controller creates mailbox workspace automatically. </li> </ul> </li> <li>Get mailbox workspace name</li> <li>Use command to register edge-syncer and obtain yaml manifests to bootstrap Edge Syncer     <pre><code>kubectl ws &lt;mb-ws name&gt;\nbin/kubectl-kcpforedgesyncer workload edge-sync &lt;Edge Sync Target name&gt; --syncer-image &lt;Edge Syncer Image&gt; -o edge-syncer.yaml\n</code></pre>     Here <code>bin/kubectl-kcpforedgesyncer</code> refers to a special variant of kcp's     kubectl plugin that includes the implementation of the functionality needed     here.  This variant, under the special name shown here, is a normal part of     the <code>bin</code> of edge-mc.     For the Edge Syncer image, please refer to the section to Build Edge Syncer image</li> <li>Deploy edge-syncer on an Edge cluster</li> <li>Syncer starts to run on the Edge cluster<ul> <li>Edge Syncer starts watching and consuming SyncerConfig</li> </ul> </li> </ol> <p>The overall diagram is as follows:</p> <p></p>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#what-edge-sync-plugin-does","title":"What edge-sync plugin does","text":"<p>In order for Syncer to sync resources between upstream (workspace) and doenstream (physical cluster), both access information are required. For the upstream access, the registration command of Syncer (<code>kubectl kcp workload edge-sync</code>) creates a service account, clusterrole, and clusterrolebinding in the workspace, and then generates kubeconfig manifest from the service account token, KCP server URL, and the server certificates. The kubeconfig manifest is embedded in a secret manifest and the secret is mount to <code>/kcp/</code> in Syncer pod. The command generates such deployment manifest as Syncer reads <code>/kcp/</code> for the upstream Kubeconfig. On the other hand, for the downstream part, in addition to the deployment manifest, the command generates a service account, role/clusterrole, rolebinding/clusterrolebinding for Syncer to access resources on the physical cluster. These resources for the downstream part are the resources to be deployed to downstream cluster. The serviceaccount is set to <code>serviceAccountName</code> in the deployment manifest.</p> <p>Note: In addtion to that, the command creates EdgeSyncConfig CRD if not exist, and creates a deffault EdgeSyncConfig resource with the name specified in the command (;<code>kubectl kcp workload edge-sync &lt;name&gt;</code>). The default EdgeSyncConfig is no longer needed since Syncer now consumes all EdgeSyncConfigs in the workspace. Furthermore, creation of EdgeSyncConfig CRD will also no longer be needed since we will switch to use SyncerConfig rather than EdgeSyncConfig in near future.</p> <p>The source code of the command is https://github.com/kcp-dev/edge-mc/blob/main/pkg/cliplugins/kcp-edge/syncer-gen/edgesync.go.</p> <p>The equivalent manual steps are as follows: 1. Generate UUID for Syncer identification     <pre><code>uuidgen | tr '[:upper:]' '[:lower:]' | read syncer_id\nsyncer_id=\"syncer-$syncer_id\"\n</code></pre> 2. Go to a workspace (It's exactly a mailbox workspace in the case of Edge MC)     <pre><code>kubectl ws create ws1 --enter\n</code></pre> 3. Create a serviceaccount in the workspace     <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: $syncer_id\nEOL\n</code></pre> 4. Create clusterrole and clusterrolebinding to bind the serviceaccount to the role     <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: $syncer_id\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- nonResourceURLs: [\"/\"]\n  verbs: [\"access\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: $syncer_id\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: $syncer_id\nsubjects:\n- apiGroup: \"\"\n  kind: ServiceAccount\n  name: $syncer_id\n  namespace: default\nEOL\n</code></pre> 5. Get the serviceaccount token that will be set in the upstream kubeconfig manifest     <pre><code>kubectl get secret -o custom-columns=\":.metadata.name\"| grep $syncer_id | read secret_name\nkubectl get secret $secret_name -o jsonpath='{.data.token}' | base64 -d | read token\n</code></pre> 6. Get the certificates that will be set in the upstream kubeconfig manifest     <pre><code>kubectl config view --minify --raw | yq \".clusters[0].cluster.certificate-authority-data\" | read cacrt\necho $cacrt\n</code></pre> 7. Get the server host and port that will be set in the upstream kubeconfig manifest     <pre><code>kubectl config view --minify --raw | yq \".clusters[0].cluster.server\" | sed -e 's|https://\\(.*\\):\\([^/]*\\)/.*|\\1 \\2|g' | read host port\necho $host, $port\n</code></pre> 8. Set some other parameters     a. server_url of KCP from host and port         <pre><code>server_url=\"https://$host:$port\"\n</code></pre>     b. downstream_namespace where Syncer Pod runs         <pre><code>downstream_namespace=\"kcp-edge-$syncer_id\"\n</code></pre>     c. Syncer image         <pre><code>image=\"quay.io/kcpedge/syncer:dev-2023-03-30\"\n</code></pre> 9. Generate manifests to bootstrap Edge Syncer     <pre><code>syncer_id=$syncer_id cacrt=$cacrt token=$token server_url=$server_url downstream_namespace=$downstream_namespace image=$image envsubst &lt; ./pkg/syncer/scripts/edge-syncer-bootstrap.template.yaml\n</code></pre>     For debug purpose, you can extract kubeconfig.yaml of the upstream     <pre><code>syncer_id=$syncer_id cacrt=$cacrt token=$token server_url=$server_url downstream_namespace=$downstream_namespace image=$image envsubst &lt; ./pkg/syncer/scripts/edge-syncer-bootstrap.template.yaml | yq e \"select(.kind == \\\"Secret\\\" and .metadata.name == \\\"$syncer_id\\\")\" | yq .stringData.kubeconfig \n</code></pre> 10. For now, EdgeSyncConfig API is required. Please create EdgeSyncConfig CRD in the workspace, if you run Syncer from the generated bootstrap manifest.     <pre><code>kubectl create ./pkg/syncer/config/crds/edge.kcp.io_edgesyncconfigs.yaml\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#deploy-workload-objects-from-edge-mc-to-edge-cluster","title":"Deploy workload objects from edge-mc to Edge cluster","text":"<p>To deploy resources to Edge clusters, create the following in workload management workspace - workload objects   - Some objects are denatured if needed.   - Other objects are as it is - APIExport/API Schema corresponding to CRD such as Kubernetes ClusterPolicyReport.   - TBD: Conversion from CRD to APIExport/APISchema could be automated by using MutatingAdmissionWebhook on workload management workspace. This automation is already available (see the sciprt here).  - EdgePlacement</p> <p></p> <p>After this, Edge-mc will put the following in the mailbox workspace. - Workload objects (both denatured one and not-denatured one) - SyncerConfig CR</p> <p>TODO: This is something we should clarify..e.g. which existing controller(s) in edge-mc will cover, or just create a new controller to handle uncovered one. @MikeSpreitzer gave the following suggestions.   - The placement translator will put the workload objects and syncer config into the mailbox workspaces.   - The placement translator will create syncer config based on the EdgePlacement objects and what they match.   - The mailbox controller will put API Binding into the mailbox workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#edgesyncconfig-will-be-replaced-to-syncerconfig","title":"EdgeSyncConfig (will be replaced to SyncerConfig)","text":"<ul> <li>The example of EdgeSycnerConfig CR is here. Its CRD is here.</li> <li>The CR here is used from edge syncer. </li> <li>The CR is placed at mb-ws to define</li> <li>object selector</li> <li>need of renaturing</li> <li>need of returning reported states of downsynced objects</li> <li>need of delete propagation for downsyncing</li> <li>The CR is managed by edge-mc (placement transformer).</li> <li>At the initial implementation before edge-mc side controller become ready, we assume SyncerConfig is on workload management workspace (wm-ws), and then which will be copied into mb-ws like other workload objects.</li> <li>This should be changed to be generated according to EdgePlacement spec. </li> <li>This CR is a placeholder for defining how edge-syncer behaves, and will be extended/splitted/merged according to further design discussion.</li> <li>One CR is initially created by the command for Edge Syncer enablement in mb-ws (<code>kubectl kcp workload edge-syncer &lt;name&gt;</code>)</li> <li>The CR name is <code>&lt;name&gt;</code> and the contents are empty.</li> <li>This name is registered in the bootstrap manifests for Edge Syncer install and Edge Syncer is told to watch the CR of this name.</li> <li>Currently Edge Syncer watches all CRs in the workspace</li> <li>Edge Syncer merges them and decides which resources are down/up synced based on the merged information. </li> <li>This behavior may be changed to only watching the default CR once Placement Translater is to be the component that generates the CR from EdgePlacement: related issue</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#syncerconfig","title":"SyncerConfig","text":"<ul> <li>The spec is defined in https://github.com/kcp-dev/edge-mc/blob/main/pkg/apis/edge/v1alpha1/syncer-config.go</li> <li><code>namespaceScope</code> field is for namespace scoped objects.<ul> <li><code>namespaces</code> is field for which namespaces to be downsynced.</li> <li><code>resources</code> is field for what resource's objects in the above namespaces are downsynced. All objects in the selected resource are downsynced.</li> </ul> </li> <li><code>clusterScope</code> field is for cluster scoped objects<ul> <li>It's an array of <code>apiVersion</code>, <code>group</code>, <code>resource</code>, and <code>objects</code>.</li> <li><code>objects</code> can be specified by wildcard (<code>*</code>) meaning all objects.</li> </ul> </li> <li><code>upsync</code> field is for upsynced objects including both namespace and cluster scoped objects.<ul> <li>It's an array of <code>apiGroup</code>, <code>resources</code>, <code>namespaces</code>, and <code>names</code>.</li> <li><code>apiGroup</code> is group.</li> <li><code>resources</code> is an array of upsynced resource.</li> <li><code>namespaces</code> is an array of namespace for namespace objects.</li> <li><code>names</code> is an array of upsynced object name. Wildcard (<code>*</code>) is available.</li> </ul> </li> <li>The example CR is https://github.com/yana1205/edge-mc/blob/support-syncer-config/test/e2e/edgesyncer/testdata/kyverno/syncer-config.yaml</li> <li>The CR is used from edge syncer</li> <li>The CR is placed in mb-ws to define</li> <li>object selector</li> <li>need of renaturing (May not scope in PoC2023q1)</li> <li>need of returning reported states of downsynced objects (May not scope in PoC2023q1)</li> <li>need of delete propagation for downsyncing (May not scope in PoC2023q1)</li> <li>The CR is managed by edge-mc (placement translator).</li> <li>At the initial implementation before edge-mc side controller become ready, we assume SyncerConfig is on workload management workspace (wm-ws), and then which will be copied into mb-ws like other workload objects.</li> <li>This should be changed to be generated according to EdgePlacement spec. </li> <li>This CR is a placeholder for defining how edge-syncer behaves, and will be extended/splitted/merged according to further design discussion.</li> <li>Currently Edge Syncer watches all CRs in the workspace</li> <li>Edge Syncer merges them and decides which resources are down/up synced based on the merged information. </li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#downsyncing","title":"Downsyncing","text":"<ul> <li>Edge syncer does downsyncing, which copy workload objects on mailbox workspace to Edge cluster</li> <li>If workload objects are deleted on mailbox workspace, the corresponding objects on the Edge cluster will be also deleted according to SyncerConfig. </li> <li>SyncerConfig specifies which objects should be downsynced.</li> <li>object selector: group, version, kind, name, namespace (for namespaced objects), label, annotation</li> <li>Cover cluster-scope objects and CRD</li> <li>CRD needs to be denatured if downsyncing is required. (May not scope in PoC2023q1 since no usage)</li> <li>Renaturing is applied if required (specified in SyncerConfig). (May not scope in PoC2023q1 since no usage)</li> <li>Current implementation is using polling to detect changes on mailbox workspace, but will be changed to use Informers. </li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#renaturing-may-not-scope-in-poc2023q1-since-no-usage","title":"Renaturing (May not scope in PoC2023q1 since no usage)","text":"<ul> <li>Edge syncer does renaturing, which converts workload objects to different forms of objects on a Edge cluster. </li> <li>The conversion rules (downstream/upstream mapping) is specified in SyncerConfig.</li> <li>Some objects need to be denatured. </li> <li>CRD needs to be denatured when conflicting with APIBinding.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#return-of-reported-state","title":"Return of reported state","text":"<ul> <li>Edge syncer return the reported state of downsynced objects at Edge cluster to the status of objects on the mailbox workspace periodically. </li> <li>TODO: Failing to returning reported state of some resources (e.g. deployment and service). Need more investigation. </li> <li>reported state returning on/off is configurable in SyncerConfig. (default is on)</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#resource-upsyncing","title":"Resource Upsyncing","text":"<ul> <li>Edge syncer does upsyncing resources at Edge cluster to the corresponding mailbox workspace periodically. </li> <li>SyncerConfig specifies which objects should be upsynced from Edge cluster.</li> <li>object selector: group, version, kind, name, namespace (for namespaced objects), label, annotation (, and more such as ownership reference?)</li> <li>Upsyncing CRD is out of scope for now. This means when upsyncing a CR, corresponding APIBinding (not CRD) is available on the mailbox workspace. This limitation might be revisited later. </li> <li>~Upsynced objects can be accessed from APIExport set on the workload management workspace bound to the mailbox workspace (with APIBinding). This access pattern might be changed when other APIs such as summarization are provided in edge-mc.~ =&gt; Upsynced objects are accessed through Mailbox informer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#feasibility-study","title":"Feasibility study","text":"<p>We will verify if the design describled here could cover the following 4 scenarios.  - I can register an edge-syncer on a Edge cluster to connect a mailbox workspace specified by name. (edge-syncer registration) - I can deploy Kyverno and its policy from mailbox workspace to Edge cluster just by using manifests (generated from Kyverno helm chart) rather than using OLM. (workload deployment by edge-syncer's downsyncing) - I can see the policy report generated at Edge cluster via API Export on workload management workspace. (resource upsyncing by edge-syncer)  - I can deploy the denatured objects on mailbox workspace to Edge cluster by renaturing them automatically in edge-syncer. (workload deployment by renaturing)</p>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#build-edge-syncer-image","title":"Build Edge Syncer image","text":"<p>Prerequisite - Install ko (https://ko.build/install/)</p>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#how-to-build-the-image-in-your-local","title":"How to build the image in your local","text":"<ol> <li><code>make build-edge-syncer-image-local</code> e.g. <pre><code>$ make build-edge-syncer-image-local\n2023/04/24 11:50:37 Using base distroless.dev/static:latest@sha256:81018475098138883b80dcc9c1242eb02b53465297724b18e88591a752d2a49c for github.com/kcp-dev/edge-mc/cmd/syncer\n2023/04/24 11:50:38 Building github.com/kcp-dev/edge-mc/cmd/syncer for linux/arm64\n2023/04/24 11:50:39 Loading ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:40 Loaded ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:40 Adding tag latest\n2023/04/24 11:50:40 Added tag latest\nEdge-Syncer image:\nko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n</code></pre> <code>ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96</code> is the image stored in your local Docker registory.</li> </ol> <p>You can also set a shell variable to the output of this Make task.</p> <p>For example <pre><code>image=`make build-edge-syncer-image-local`\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/edge-syncer/#how-to-build-the-image-with-multiple-architectures-and-push-it-to-docker-registory","title":"How to build the image with multiple architectures and push it to Docker registory","text":"<ol> <li><code>make build-edge-syncer-image DOCKER_REPO=ghcr.io/yana1205/edge-mc/syncer IMAGE_TAG=dev-2023-04-24-x ARCHS=linux/amd64,linux/arm64</code></li> </ol> <p>For example <pre><code>$ make build-edge-syncer-image DOCKER_REPO=ghcr.io/yana1205/edge-mc/syncer IMAGE_TAG=dev-2023-04-24-x ARCHS=linux/amd64,linux/arm64\n2023/04/24 11:50:16 Using base distroless.dev/static:latest@sha256:81018475098138883b80dcc9c1242eb02b53465297724b18e88591a752d2a49c for github.com/kcp-dev/edge-mc/cmd/syncer\n2023/04/24 11:50:17 Building github.com/kcp-dev/edge-mc/cmd/syncer for linux/arm64\n2023/04/24 11:50:17 Building github.com/kcp-dev/edge-mc/cmd/syncer for linux/amd64\n2023/04/24 11:50:18 Publishing ghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x\n2023/04/24 11:50:19 existing blob: sha256:85a5162a65b9641711623fa747dab446265400043a75c7dfa42c34b740dfdaba\n2023/04/24 11:50:20 pushed blob: sha256:00b7b3ca30fa5ee9336a9bc962efef2001c076a3149c936b436f409df710b06f\n2023/04/24 11:50:21 ghcr.io/yana1205/edge-mc/syncer:sha256-a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510.sbom: digest: sha256:4b1407327a486c0506188b67ad24222ed7924ba57576e47b59a4c1ac73dacd40 size: 368\n2023/04/24 11:50:21 Published SBOM ghcr.io/yana1205/edge-mc/syncer:sha256-a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510.sbom\n2023/04/24 11:50:21 existing blob: sha256:930413008565fd110e7ab2d37aab538449f058e7d83e7091d1aa0930a0086f58\n2023/04/24 11:50:22 pushed blob: sha256:bd830efcc6c0a934a273202ffab27b1a8927368a7b99c4ae0cf850fadb865ead\n2023/04/24 11:50:23 ghcr.io/yana1205/edge-mc/syncer:sha256-02db9874546b79ee765611474eb647128292e8cda92f86ca1b7342012eb79abe.sbom: digest: sha256:5c79e632396b893c3ecabf6b9ba43d8f20bb3990b0c6259f975bf81c63f0e41e size: 369\n2023/04/24 11:50:23 Published SBOM ghcr.io/yana1205/edge-mc/syncer:sha256-02db9874546b79ee765611474eb647128292e8cda92f86ca1b7342012eb79abe.sbom\n2023/04/24 11:50:24 existing blob: sha256:bb5ef9628a98afa48a9133f5890c43ed1499eb82a33fe173dd9067d7a9cdfb0a\n2023/04/24 11:50:25 pushed blob: sha256:61f19080792ae91e8b37ecf003376497b790a411d7a8fa4435c7457b0e15874c\n2023/04/24 11:50:25 ghcr.io/yana1205/edge-mc/syncer:sha256-c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96.sbom: digest: sha256:8d82388bb534933d7193c661743fca8378cc561a2ad8583c0107f687acb37c1b size: 369\n2023/04/24 11:50:25 Published SBOM ghcr.io/yana1205/edge-mc/syncer:sha256-c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96.sbom\n2023/04/24 11:50:26 existing manifest: sha256:02db9874546b79ee765611474eb647128292e8cda92f86ca1b7342012eb79abe\n2023/04/24 11:50:26 existing manifest: sha256:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:27 ghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x: digest: sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510 size: 690\n2023/04/24 11:50:27 Published ghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x@sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510\necho KO_DOCKER_REPO=ghcr.io/yana1205/edge-mc/syncer ko build --platform=linux/amd64,linux/arm64 --bare --tags ./cmd/syncer\nKO_DOCKER_REPO=ghcr.io/yana1205/edge-mc/syncer ko build --platform=linux/amd64,linux/arm64 --bare --tags ./cmd/syncer\nEdge-Syncer image\nghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x@sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510\n</code></pre> <code>ghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x@sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510</code> is the image pushed to the registry.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/","title":"2023q1 PoC Example Scenario","text":"<p>This doc attempts to show a simple example usage of the 2023q1 PoC. This doc is a work in progress.</p> <p>This example involves two edge clusters and two workloads.  One workload goes on both edge clusters and one workload goes on only one edge cluster.  Nothing changes after the initial activity.</p> <p>This example is presented in stages.  The controllers involved are always maintaining relationships.  This document focuses on changes as they appear in this example.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-1","title":"Stage 1","text":"<p>Stage 1 creates the infrastructure and the edge service provider workspace and lets that react to the inventory.  Then the edge syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox workspaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\nextraPortMappings:\n- containerPort: 8081\nhostPort: 8081\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\nextraPortMappings:\n- containerPort: 8081\nhostPort: 8096\n- containerPort: 8082\nhostPort: 8097\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.</p> <p>In some shell that will be used only for this purpose, issue the <code>kcp start</code> command.  If you have junk from previous runs laying around, you should probably <code>rm -rf .kcp</code> first.</p> <p>In the shell commands in all the following steps it is assumed that <code>kcp</code> is running and <code>$KUBECONFIG</code> is set to the <code>.kcp/admin.kubeconfig</code> that <code>kcp</code> produces, except where explicitly noted that the florin or guilder cluster is being accessed.</p> <p>It is also assumed that you have the usual kcp kubectl plugins on your <code>$PATH</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-an-inventory-management-workspace","title":"Create an inventory management workspace.","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create imw-1 --enter\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#get-edge-mc","title":"Get edge-mc","text":"<p>Download and build or install edge-mc, according to your preference.  That is, either (a) <code>git clone</code> the repo and then <code>make build</code> to populate its <code>bin</code> directory, or (b) fetch the binary archive appropriate for your machine from a release and unpack it (creating a <code>bin</code> directory).  In the following exhibited command lines, the commands described as \"edge-mc commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the edge-mc <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames.  The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=si</code>.</p> <pre><code>kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nkubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=si\n</code></pre> <p>Those two script invocations are equivalent to creating the following four objects.</p> <pre><code>apiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: si\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: si\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-the-edge-service-provider-workspace","title":"Create the edge service provider workspace","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create espw --enter\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#populate-the-edge-service-provider-workspace","title":"Populate the edge service provider workspace","text":"<p>This puts the definition and export of the edge-mc API in the edge service provider workspace.</p> <p>Use the following command.</p> <pre><code>kubectl create -f config/exports\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#the-mailbox-controller","title":"The mailbox controller","text":"<p>Running the mailbox controller will be conveniently automated. Eventually.  In the meantime, you can use the edge-mc command shown here.</p> <pre><code>$ mailbox-controller -v=2\n...\nI0423 01:09:37.991080   10624 main.go:196] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\n...\nI0423 01:09:38.449395   10624 controller.go:299] \"Created APIBinding\" worker=1 mbwsName=\"apmziqj9p9fqlflm-mb-bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\" mbwsCluster=\"yk9a66vjms1pi8hu\" bindingName=\"bind-edge\" resourceVersion=\"914\"\n...\nI0423 01:09:38.842881   10624 controller.go:299] \"Created APIBinding\" worker=3 mbwsName=\"apmziqj9p9fqlflm-mb-b8c64c64-070c-435b-b3bd-9c0f0c040a54\" mbwsCluster=\"12299slctppnhjnn\" bindingName=\"bind-edge\" resourceVersion=\"968\"\n^C\n</code></pre> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>This controller creates a mailbox workspace for each SyncTarget and puts an APIBinding to the edge API in each of those mailbox workspaces.  For this simple scenario, you do not need to keep this controller running after it does those things (hence the <code>^C</code> above); normally it would run continuously.</p> <p>You can get a listing of those mailbox workspaces as follows.</p> <pre><code>$ kubectl get Workspaces\nNAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal            Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal            Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\n</code></pre> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <pre><code>$ kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kcp\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\nNAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\n</code></pre> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <pre><code>$ kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"guilder\") | .name'\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#connect-guilder-edge-cluster-with-its-mailbox-workspace","title":"Connect guilder edge cluster with its mailbox workspace","text":"<p>The following command will (a) create, in the mailbox workspace for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <pre><code>$ kubectl kubestellar prep-for-syncer --imw root:imw-1 guilder\nCurrent workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\nCreating service account \"kcp-edge-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kcp-edge-syncer-guilder-wfeig2lv\" to give service account \"kcp-edge-syncer-guilder-wfeig2lv\"\n 1. write and sync access to the synctarget \"kcp-edge-syncer-guilder-wfeig2lv\"\n 2. write access to apiresourceimports.\nCreating or updating cluster role binding \"kcp-edge-syncer-guilder-wfeig2lv\" to bind service account \"kcp-edge-syncer-guilder-wfeig2lv\" to cluster role \"kcp-edge-syncer-guilder-wfeig2lv\".\nWrote physical cluster manifest to guilder-syncer.yaml for namespace \"kcp-edge-syncer-guilder-wfeig2lv\". Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kcp-edge-syncer-guilder-wfeig2lv\" kcp-edge-syncer-guilder-wfeig2lv\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <pre><code>$ KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\nnamespace/kcp-edge-syncer-guilder-wfeig2lv created\nserviceaccount/kcp-edge-syncer-guilder-wfeig2lv created\nsecret/kcp-edge-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kcp-edge-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kcp-edge-syncer-guilder-wfeig2lv created\nsecret/kcp-edge-syncer-guilder-wfeig2lv created\ndeployment.apps/kcp-edge-syncer-guilder-wfeig2lv created\n</code></pre> <p>You might check that the syncer is running, as follows.</p> <pre><code>$ KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\nNAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nkcp-edge-syncer-guilder-saaywsu5   kcp-edge-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                        coredns                            2/2     2            2           35m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           35m\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#connect-florin-edge-cluster-with-its-mailbox-workspace","title":"Connect florin edge cluster with its mailbox workspace","text":"<p>Do the analogous stuff for the florin cluster.</p> <pre><code>$ kubectl kubestellar prep-for-syncer --imw root:imw-1 florin\nCurrent workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\nCreating service account \"kcp-edge-syncer-florin-32uaph9l\"\nCreating cluster role \"kcp-edge-syncer-florin-32uaph9l\" to give service account \"kcp-edge-syncer-florin-32uaph9l\"\n 1. write and sync access to the synctarget \"kcp-edge-syncer-florin-32uaph9l\"\n 2. write access to apiresourceimports.\nCreating or updating cluster role binding \"kcp-edge-syncer-florin-32uaph9l\" to bind service account \"kcp-edge-syncer-florin-32uaph9l\" to cluster role \"kcp-edge-syncer-florin-32uaph9l\".\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kcp-edge-syncer-florin-32uaph9l\". Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kcp-edge-syncer-florin-32uaph9l\" kcp-edge-syncer-florin-32uaph9l\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre> <p>And deploy the syncer in the florin cluster.</p> <pre><code>$ KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml namespace/kcp-edge-syncer-florin-32uaph9l created\nserviceaccount/kcp-edge-syncer-florin-32uaph9l created\nsecret/kcp-edge-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kcp-edge-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kcp-edge-syncer-florin-32uaph9l created\nsecret/kcp-edge-syncer-florin-32uaph9l created\ndeployment.apps/kcp-edge-syncer-florin-32uaph9l created\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the scheduler react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a common parent for those two workspaces, with the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create my-org --enter\n</code></pre> <p>Next, create the WMW for the common workload.  The following command will do that, if issued while \"root:my-org\" is the current workspace.</p> <pre><code>kubectl kubestellar ensure wmw wmw-c\n</code></pre> <p>This is equivalent to creating that workspace and then entering it and creating the following two <code>APIBinding</code> objects.</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-espw\nspec:\nreference:\nexport:\npath: root:espw\nname: edge.kcp.io\n---\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-kube\nspec:\nreference:\nexport:\npath: \"root:compute\"\nname: kubernetes\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  namespaceSelector:\n    matchLabels: {\"common\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code>kubectl ws root:my-org\nkubectl kubestellar ensure wmw wmw-s\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"si\"}\n  namespaceSelector: \n    matchLabels: {\"special\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#edge-scheduling","title":"Edge scheduling","text":"<p>In response to each EdgePlacement, the scheduler will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>Eventually there will be automation that conveniently runs the scheduler.  In the meantime, you can run it by hand: switch to the ESPW and invoke the edge-mc command that runs the scheduler.</p> <pre><code>$ kubectl ws root:espw\nCurrent workspace is \"root:espw\".\n$ kubestellar-scheduler\nI0423 01:33:37.036752   11305 kubestellar-scheduler.go:212] \"Found APIExport view\" exportName=\"edge.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/7qkse309upzrv0fy/edge.kcp.io\"\n...\nI0423 01:33:37.320859   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|florin\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"florin\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n...\nI0423 01:33:37.391772   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|guilder\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"guilder\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n^C\n</code></pre> <p>In this simple scenario you do not need to keep the scheduler running after it gets its initial work done; normally it would run continually.</p> <p>Check out the SinglePlacementSlice objects as follows.</p> <pre><code>$ kubectl ws root:my-org:wmw-c\nCurrent workspace is \"root:my-org:wmw-c\".\n$ kubectl get SinglePlacementSlice -o yaml\napiVersion: v1\nitems:\n- apiVersion: edge.kcp.io/v1alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\n    generation: 4\n    name: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kcp.io/v1alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\n    uid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre> <p>Also check out the SinglePlacementSlice objects in <code>root:my-org:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox workspaces and create <code>SyncerConfig</code> objects there.</p> <p>Eventually there will be convenient automation running the placement translator.  In the meantime, you can run it manually: switch to the ESPW and use the edge-mc command that runs the placement translator.</p> <pre><code>$ kubectl ws root:espw\nCurrent workspace is \"root:espw\".\n$ placement-translator\nI0423 01:39:56.362722   11644 shared_informer.go:282] Waiting for caches to sync for placement-translator\n...\n</code></pre> <p>After it stops logging stuff, wait another minute and then you can ^C it or use another shell to continue exploring.</p> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.</p> <pre><code>$ kubectl ws 1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\n\n$ kubectl get SyncerConfig the-one -o yaml\napiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    resources:\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>You can check that the workload got there too.</p> <pre><code>$ kubectl get ns\nNAME          STATUS   AGE\ncommonstuff   Active   6m34s\ndefault       Active   32m\n$ kubectl get deployments -A\nNAMESPACE     NAME      READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff   commond   0/0     0            0           6m44s\n</code></pre> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows.</p> <pre><code>$ kubectl ws root:espw\nCurrent workspace is \"root:espw\".\n$ kubectl ws 1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\n$ kubectl get SyncerConfig the-one -o yaml\napiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\n  generation: 4\n  name: the-one\n  resourceVersion: \"1325\"\n  uid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\n      resource: services\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\n      resource: pods\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: \"\"\n      resource: limitranges\n    - apiVersion: v1\n      group: \"\"\n      resource: serviceaccounts\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: \"\"\n      resource: configmaps\n    - apiVersion: v1\n      group: \"\"\n      resource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\n      resource: resourcequotas\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\n    resources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n$ kubectl get deployments -A\nNAMESPACE      NAME       READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff    commond    0/0     0            0           6m1s\nspecialstuff   speciald   0/0     0            0           5m58s\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <pre><code>$ KUBECONFIG=~/.kube/config kubectl --context kind-florin get ns\nNAME                              STATUS   AGE\ncommonstuff                       Active   6m51s\ndefault                           Active   57m\nkcp-edge-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                   Active   57m\nkube-public                       Active   57m\nkube-system                       Active   57m\nlocal-path-storage                Active   57m\n$ KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A | egrep 'NAME|stuff'\nNAMESPACE                         NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                       commond                           1/1     1            1           7m59s\n</code></pre> <p>Examine the guilder cluster.  Find both workload namespaces and both Deployments.</p> <pre><code>$ KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\nNAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n$ KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A | egrep NAME\\|stuff\nNAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                        commond                            1/1     1            1           8m37s\nspecialstuff                       speciald                           1/1     1            1           8m55s\n</code></pre> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <pre><code>$ kubectl --context kind-guilder get deploy -n commonstuff commond -o yaml\n...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre> <p>Check that the common workload on the florin cluster is working.</p> <pre><code>$ curl http://localhost:8081\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Check that the special workload on the guilder cluster is working.</p> <pre><code>$ curl http://localhost:8097\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Check that the common workload on the guilder cluster is working.</p> <pre><code>$ curl http://localhost:8096\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-5","title":"Stage 5","text":"<p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the special workload, creates a status summary object in the specialstuff namespace in the special workload workspace holding a summary of the corresponding Deployment objects. In this case there is just one such object, in the mailbox workspace for the guilder cluster.</p> <p></p> <p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the common workload, creates a status summary object in the commonstuff namespace in the common workload workspace holding a summary of the corresponding Deployment objects.  Those are the <code>commond</code> Deployment objects in the two mailbox workspaces.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/","title":"KubeStellar Scheduler","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#pre-requisite","title":"Pre-requisite:","text":"<p>You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19. <pre><code>brew install go@1.19\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#steps-to-try-the-scheduler","title":"Steps to try the scheduler","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#pull-the-kcp-and-kubestellar-source-code-build-the-kubectl-ws-binary-and-start-kcp","title":"Pull the kcp and KubeStellar source code, build the kubectl-ws binary, and start kcp","text":"<p>open a terminal window(1) and clone the latest KubeStellar source: <pre><code>git clone https://github.com/kcp-dev/edge-mc KubeStellar\n</code></pre></p> <p>clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>cd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>run kcp (kcp will spit out tons of information and stay running in this terminal window) <pre><code>kcp start\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#create-the-edge-service-provider-workspace-espw-and-populate-it-with-crds-and-apis","title":"Create the Edge Service Provider Workspace (ESPW) and populate it with CRDs and APIs","text":"<p>open another terminal window(2) and point <code>$KUBECONFIG</code> to the admin kubeconfig for the kcp server and include the location of kubectl-ws in <code>$PATH</code>. <pre><code>export KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Use workspace <code>root:edge</code> as the Edge Service Provider Workspace (ESPW). <pre><code>kubectl ws root\nkubectl ws create edge --enter\n</code></pre></p> <p>Install CRDs and APIExport. <pre><code>kubectl apply -f ../KubeStellar/config/exports/\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#create-the-workload-management-workspace-wmw-and-bind-it-to-the-espw-apis","title":"Create the Workload Management Workspace (WMW) and bind it to the ESPW APIs","text":"<p>Use the user home workspace (<code>~</code>) as the workload management workspace (WMW). <pre><code>kubectl ws ~\n</code></pre></p> <p>Bind APIs. <pre><code>kubectl apply -f ../KubeStellar/config/imports/\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#run-the-kubestellar-scheduler-against-the-espw","title":"Run the KubeStellar Scheduler against the ESPW","text":"<p>Go to <code>root:edge</code> workspace and run the edge scheduler. <pre><code>kubectl ws root:edge\ncd ../KubeStellar\ngo run cmd/kubestellar-scheduler/main.go -v 2\n</code></pre></p> <p>The outputs from the edge scheduler should be similar to: <pre><code>I0327 17:14:42.222112   51241 kubestellar-scheduler.go:243] \"Found APIExport view\" exportName=\"edge.kcp.io\" serverURL=\"https://192.168.1.54:6443/services/apiexport/291lkbsqq181xfng/edge.kcp.io\"\nI0327 17:14:42.225075   51241 kubestellar-scheduler.go:243] \"Found APIExport view\" exportName=\"scheduling.kcp.io\" serverURL=\"https://192.168.1.54:6443/services/apiexport/root/scheduling.kcp.io\"\nI0327 17:14:42.226954   51241 kubestellar-scheduler.go:243] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.1.54:6443/services/apiexport/root/workload.kcp.io\"\nI0327 17:14:42.528573   51241 controller.go:201] \"starting controller\" controller=\"kubestellar-scheduler\"\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#create-the-inventory-management-workspace-imw-and-populate-it-with-locations-and-synctargets","title":"Create the Inventory Management Workspace (IMW) and populate it with locations and synctargets","text":"<p>open another terminal window(3) and point <code>$KUBECONFIG</code> to the admin kubeconfig for the kcp server and include the location of kubectl-ws in $PATH. <pre><code>cd ../kcp\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Use workspace <code>root:compute</code> as the Inventory Management Workspace (IMW). <pre><code>kubectl ws root:compute\n</code></pre></p> <p>Create two Locations and two SyncTargets. <pre><code>kubectl create -f ../KubeStellar/config/samples/location_prod.yaml\nkubectl create -f ../KubeStellar/config/samples/location_dev.yaml\nkubectl create -f ../KubeStellar/config/samples/synctarget_prod.yaml\nkubectl create -f ../KubeStellar/config/samples/synctarget_dev.yaml\n</code></pre></p> <p>Note that kcp automatically creates a Location <code>default</code>. So there are 3 Locations and 2 SyncTargets in <code>root:compute</code>. <pre><code>kubectl get locations,synctargets\nNAME                                 RESOURCE      AVAILABLE   INSTANCES   LABELS   AGE\nlocation.scheduling.kcp.io/default   synctargets   0           2                    2m12s\nlocation.scheduling.kcp.io/dev       synctargets   0           1                    2m39s\nlocation.scheduling.kcp.io/prod      synctargets   0           1                    3m13s\nNAME                              AGE\nsynctarget.workload.kcp.io/dev    110s\nsynctarget.workload.kcp.io/prod   2m12s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#create-some-edgeplacements-in-the-wmw","title":"Create some EdgePlacements in the WMW","text":"<p>Go to Workload Management Workspace (WMW) and create an EdgePlacement <code>test-1</code>. <pre><code>kubectl ws ~\nkubectl create -f ../KubeStellar/config/samples/edgeplacement_test-1.yaml\n</code></pre></p> <p>The scheduler maintains a SinglePlacementSlice for an EdgePlacement in the same workspace. <pre><code>kubectl get sps test-1 -oyaml\napiVersion: edge.kcp.io/v1alpha1\ndestinations:\n- cluster: f3il38atqno12hfd\n  locationName: prod\n  syncTargetName: prod\n  syncTargetUID: 8c0a7003-ad18-4bf0-90a0-b1d74cda2437\n- cluster: f3il38atqno12hfd\n  locationName: dev\n  syncTargetName: dev\n  syncTargetUID: dc490a42-e8f1-4930-a142-6c0ba8fd39d5\n- cluster: f3il38atqno12hfd\n  locationName: default\n  syncTargetName: prod\n  syncTargetUID: 8c0a7003-ad18-4bf0-90a0-b1d74cda2437\n- cluster: f3il38atqno12hfd\n  locationName: default\n  syncTargetName: dev\n  syncTargetUID: dc490a42-e8f1-4930-a142-6c0ba8fd39d5\nkind: SinglePlacementSlice\nmetadata:\n  annotations:\n    kcp.io/cluster: kvdk2spgmbix\n  creationTimestamp: \"2023-03-27T21:37:29Z\"\n  generation: 1\n  name: test-1\n  ownerReferences:\n  - apiVersion: edge.kcp.io/v1alpha1\n    kind: EdgePlacement\n    name: test-1\n    uid: 0c68724e-6d11-4cff-bd0a-8fa32c86caa9\n  resourceVersion: \"877\"\n  uid: 45ec86d7-bdf8-4c2d-bc02-087073a1ac17\n</code></pre> EdgePlacement <code>test-1</code> selects all the 3 Locations in <code>root:compute</code>.</p> <p>Create a more specific EdgePlacement which selects Locations labeled by <code>env: dev</code>. <pre><code>kubectl create -f ../KubeStellar/config/samples/edgeplacement_dev.yaml\n</code></pre></p> <p>The corresponding SinglePlacementSlice has a shorter list of <code>destinations</code>: <pre><code>kubectl get sps dev -oyaml\napiVersion: edge.kcp.io/v1alpha1\ndestinations:\n- cluster: f3il38atqno12hfd\n  locationName: dev\n  syncTargetName: dev\n  syncTargetUID: dc490a42-e8f1-4930-a142-6c0ba8fd39d5\nkind: SinglePlacementSlice\nmetadata:\n  annotations:\n    kcp.io/cluster: kvdk2spgmbix\n  creationTimestamp: \"2023-03-27T21:40:52Z\"\n  generation: 1\n  name: dev\n  ownerReferences:\n  - apiVersion: edge.kcp.io/v1alpha1\n    kind: EdgePlacement\n    name: dev\n    uid: 6e9d608d-12cd-47cc-8887-3695199259ba\n  resourceVersion: \"880\"\n  uid: 9b8de087-21bc-4585-99cb-e6c03ba0a8ae\n</code></pre></p> <p>Feel free to change the Locations, SyncTargets, and EdgePlacements and see how the scheduler reacts.</p> <p>Your next step is to deliver a workload to a mailbox (that represents an edge location).  Go here to take the next step... (TBD)</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/","title":"Mailbox Controller","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#linking-synctarget-with-mailbox-workspace","title":"Linking SyncTarget with Mailbox Workspace","text":"<p>For a given SyncTarget T, the mailbox controller currently chooses the name of the corresponding workspace to be the concatenation of the following.</p> <ul> <li>the ID of the logical cluster containing T</li> <li>the string \"-mb-\"</li> <li>T's UID</li> </ul> <p>The mailbox workspace gets an annotation whose key is <code>edge.kcp.io/sync-target-name</code> and whose value is the name of the workspace object (as seen in its parent workspace, the edge service provider workspace).</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#usage","title":"Usage","text":"<p>The mailbox controller needs three Kubernetes client configurations. One --- concerned with reading inventory --- is to access the APIExport view of the <code>workload.kcp.io</code> API group, to read the <code>SyncTarget</code> objects.  This must be a client config that is pointed at the workspace (which is always <code>root</code>, as far as I know) that has this APIExport and is authorized to read its view.  Another client config is needed to give read/write access to all the mailbox workspaces, so that the controller can create <code>APIBinding</code> objects to the edge APIExport in those workspaces; this should be a cliet config that is able to read/write in all clusters.  For example, that is in the kubeconfig context named <code>base</code> in the kubeconfig created by <code>kcp start</code>.  Finally, the controller also needs a kube client config that is pointed at the edge service provider workspace and is authorized to consume the <code>Workspace</code> objects from there.</p> <p>The command line flags, beyond the basics, are as follows.</p> <pre><code>      --concurrency int                  number of syncs to run in parallel (default 4)\n      --espw-path string                 the pathname of the edge service provider workspace (default \"root:espw\")\n      --inventory-cluster string         The name of the kubeconfig cluster to use for access to APIExport view of SyncTarget objects\n      --inventory-context string         The name of the kubeconfig context to use for access to APIExport view of SyncTarget objects (default \"root\")\n      --inventory-kubeconfig string      Path to the kubeconfig file to use for access to APIExport view of SyncTarget objects\n      --inventory-user string            The name of the kubeconfig user to use for access to APIExport view of SyncTarget objects\n      --mbws-cluster string              The name of the kubeconfig cluster to use for access to mailbox workspaces (really all clusters)\n      --mbws-context string              The name of the kubeconfig context to use for access to mailbox workspaces (really all clusters) (default \"base\")\n      --mbws-kubeconfig string           Path to the kubeconfig file to use for access to mailbox workspaces (really all clusters)\n      --mbws-user string                 The name of the kubeconfig user to use for access to mailbox workspaces (really all clusters)\n      --server-bind-address ipport       The IP address with port at which to serve /metrics and /debug/pprof/ (default :10203)\n      --workload-cluster string          The name of the kubeconfig cluster to use for access to edge service provider workspace\n      --workload-context string          The name of the kubeconfig context to use for access to edge service provider workspace\n      --workload-kubeconfig string       Path to the kubeconfig file to use for access to edge service provider workspace\n      --workload-user string             The name of the kubeconfig user to use for access to edge service provider workspace\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#try-it","title":"Try It","text":"<p>To exercise it, do the following steps.</p> <p>Clone this repo, install kcp (the version for <code>github.com/kcp-dev/kcp</code> in <code>go.mod</code> is required)  and start a kcp server as described here.</p> <p>Do the remaining steps in a separate shell, with <code>$KUBECONFIG</code> set to the admin config for that kcp server.  </p> <p>Next, create the edge service provider workspace:</p> <pre><code>kubectl ws root\nkubectl ws create espw --enter\n</code></pre> <p>After that, a run of the controller should look like the following.</p> <pre><code>$ go run ./cmd/mailbox-controller -v=2\nI0305 18:06:20.046741   85556 main.go:110] \"Command line flag\" add_dir_header=\"false\"\nI0305 18:06:20.046954   85556 main.go:110] \"Command line flag\" alsologtostderr=\"false\"\nI0305 18:06:20.046960   85556 main.go:110] \"Command line flag\" concurrency=\"4\"\nI0305 18:06:20.046965   85556 main.go:110] \"Command line flag\" inventory-context=\"root\"\nI0305 18:06:20.046971   85556 main.go:110] \"Command line flag\" inventory-kubeconfig=\"\"\nI0305 18:06:20.046976   85556 main.go:110] \"Command line flag\" log_backtrace_at=\":0\"\nI0305 18:06:20.046980   85556 main.go:110] \"Command line flag\" log_dir=\"\"\nI0305 18:06:20.046985   85556 main.go:110] \"Command line flag\" log_file=\"\"\nI0305 18:06:20.046989   85556 main.go:110] \"Command line flag\" log_file_max_size=\"1800\"\nI0305 18:06:20.046993   85556 main.go:110] \"Command line flag\" logtostderr=\"true\"\nI0305 18:06:20.046997   85556 main.go:110] \"Command line flag\" one_output=\"false\"\nI0305 18:06:20.047002   85556 main.go:110] \"Command line flag\" server-bind-address=\":10203\"\nI0305 18:06:20.047006   85556 main.go:110] \"Command line flag\" skip_headers=\"false\"\nI0305 18:06:20.047011   85556 main.go:110] \"Command line flag\" skip_log_headers=\"false\"\nI0305 18:06:20.047015   85556 main.go:110] \"Command line flag\" stderrthreshold=\"2\"\nI0305 18:06:20.047019   85556 main.go:110] \"Command line flag\" v=\"2\"\nI0305 18:06:20.047023   85556 main.go:110] \"Command line flag\" vmodule=\"\"\nI0305 18:06:20.047027   85556 main.go:110] \"Command line flag\" workload-context=\"\"\nI0305 18:06:20.047031   85556 main.go:110] \"Command line flag\" workload-kubeconfig=\"\"\nI0305 18:06:20.070071   85556 main.go:247] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\nI0305 18:06:20.072088   85556 shared_informer.go:282] Waiting for caches to sync for mailbox-controller\nI0305 18:06:20.172169   85556 shared_informer.go:289] Caches are synced for mailbox-controller\nI0305 18:06:20.172196   85556 main.go:210] \"Informers synced\"\n</code></pre> <p>In a separate shell, make a inventory management workspace as follows.</p> <pre><code>kubectl ws \\~\nkubectl ws create imw --enter\n</code></pre> <p>Then in that workspace, run the following command to create a <code>SyncTarget</code> object.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\n  name: stest1\nspec:\n  cells:\n    foo: bar\nEOF\n</code></pre> <p>That should provoke logging like the following from the mailbox controller.</p> <pre><code>I0305 18:07:20.490417   85556 main.go:369] \"Created missing workspace\" worker=0 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre> <p>And you can verify that as follows:</p> <pre><code>$ kubectl ws root:espw\nCurrent workspace is \"root:espw\".\n$ kubectl get workspaces\nNAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\nniqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368   universal            Ready   https://192.168.58.123:6443/clusters/0ay27fcwuo2sv6ht   22s\n</code></pre> <p>FYI, if you look inside that workspace you will see an <code>APIBinding</code> named <code>bind-edge</code> that binds to the <code>APIExport</code> named <code>edge.kcp.io</code> from the edge service provider workspace (and this is why the controller needs to know the pathname of that workspace), so that the edge API is available in the mailbox workspace.</p> <p>Next, <code>kubectl delete</code> that workspace, and watch the mailbox controller wait for it to be gone and then re-create it.</p> <pre><code>I0305 18:08:15.428884   85556 main.go:369] \"Created missing workspace\" worker=2 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre> <p>Finally, go back to your inventory workspace to delete the <code>SyncTarget</code>:</p> <p><pre><code>kubectl ws \\~\nkubectl ws imw\nkubectl delete SyncTarget stest1\n</code></pre> and watch the mailbox controller react as follows.</p> <pre><code>I0305 18:08:44.380421   85556 main.go:352] \"Deleted unwanted workspace\" worker=0 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/outline/","title":"Details","text":"<p>Want to get involved? Check out our good-first-issue list.</p> <p></p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#status-of-this-memo","title":"Status of this memo","text":"<p>This summarizes the current state of design work that is still in progress.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#introduction","title":"Introduction","text":"<p>This is a quick demo of a fragment of what we think is needed for edge multi-cluster.  It is intended to demonstrate the following points.</p> <ul> <li>Separation of infrastructure and workload management.</li> <li>The focus here is on workload management, and that strictly reads   an inventory of infrastructure.</li> <li>What passes from inventory to workload management is kcp TMC   Location and SyncTarget objects.</li> <li>Use of a kcp workspace as the container for the central spec of a workload.</li> <li>Propagation of desired state from center to edge, as directed by   EdgePlacement objects and the Location and SyncTarget objects they reference.</li> <li>Interfaces designed for a large number of edge clusters.</li> <li>Interfaces designed with the intention that edge clusters operate   independently of each other and the center (e.g., can tolerate only   occasional connectivity) and thus any \"service providers\" (in the   technical sense from kcp) in the center or elsewhere.</li> <li>Rule-based customization of desired state.</li> <li>Propagation of reported state from edge to center.</li> <li>Summarization of reported state in the center.</li> <li>Return and/or summarization of state from associated objects (e.g.,   ReplicaSet or Pod objects associated with a given Deployment   object).</li> <li>The edge opens connections to the center, not vice-versa.</li> <li>An edge computing platform \"product\" that can be deployed (as   opposed to a service that is used).</li> </ul> <p>Some important things that are not attempted in this PoC include the following.</p> <ul> <li>An implementation that supports a large number of edge clusters or   any other thing that requires sharding for scale. In this PoC we   will use a single kcp server to hold all the workspaces, and will   not shard any controller.</li> <li>More than one SyncTarget per Location.</li> <li>A hierarchy with more than two levels.</li> <li>User control over ordering of propagation from center to edge,   either among destinations or kinds of objects.</li> <li>More than baseline security (baseline being, e.g., HTTPS, Secret   objects, non-rotating bearer token based service authentication).</li> <li>A good design for bootstrapping the workload management in the edge   clusters.</li> <li>Very strong isolation between tenants in the edge computing   platform.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#development-roadmap","title":"Development Roadmap","text":"<p>Some features will get implemented later than others, so that we can start being able to run interesting end-to-end scenarios relatively soon.  Following is a list of features that will not be implemented at first.</p> <p>Of the following features, customization will be needed before the others.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#customization","title":"Customization","text":"<p>We can have a complete system that ignores customization, as long as it is only used for workloads that need no customization.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#summarization","title":"Summarization","text":"<p>We can omit summarization at first.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#return-andor-summarization-of-state-from-associated-objects","title":"Return and/or summarization of state from associated objects","text":"<p>This will involve both defining a scalable interface for declaring what should be returned as well as implementing it.  This will certainly affect the syncer between mailbox workspace and edge cluster, and the summarization part will affect the status summarizer.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#good-handling-of-workload-conflicts","title":"Good handling of workload conflicts","text":"<p>We could start by handling workload conflicts in a very simple way: treating each as an error.  Later development can handle them better, as outlined later.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#denaturingrenaturing","title":"Denaturing/renaturing","text":"<p>We could start by not doing this.  For some resources, the effect of leaving these resources natured in the center is only to add authorizations in the center that are not needed and are undesired in a well-secured environment but tolerable in early demonstrations --- provided that there is not a conflict with an object of the same name that is positively desired in the center.  In particular, these are: <code>ClusterRole</code>, <code>ClusterRoleBinding</code>, <code>Role</code>, <code>RoleBinding</code>, and, depending on the Kubernetes release and usage style, <code>ServiceAccount</code>. The extra consideration for <code>ServiceAccount</code> is when an associated <code>Secret</code> is a natural consequence.  However, that is not a practical problem because such <code>Secret</code> objects are recognized as system infrastructure (see below).  Another consideration for <code>ServiceAccount</code> objects, as for <code>Secret</code> and <code>ConfigMap</code> objects, is that some are in some sense \"reverse-natured\": some are created by some other thing as part of the nature of that other thing (object or external system).  Another way of looking at these particular objects is that they are system infrastructure.</p> <p>For some kinds of object, lack of denaturing/renaturing means that edge-mc will simply not be able to support workloads that contain such objects.  These are: <code>MutatingWebhookConfiguration</code>, <code>ValidatingWebhookConfiguration</code>, <code>LimitRange</code>, <code>ResourceQuota</code>.</p> <p>For some resources, the need to denature is only a matter of anticipation.  <code>FlowSchema</code> and <code>PriorityLevelConfiguration</code> currently are not interpreted by kcp servers and so are effectively already denatured there.  Hopefully they will be given interpretations in the future, and then those resources will join the previous category.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#roles-and-responsibilities","title":"Roles and Responsibilities","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#developersdeployersadminsusers-of-the-infrastructure-management-layer","title":"Developers/deployers/admins/users of the infrastructure management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#developers-of-the-workload-management-layer","title":"Developers of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#deployersadmins-of-the-workload-management-layer","title":"Deployers/admins of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#users-of-the-workload-management-layer","title":"Users of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#design-overview","title":"Design overview","text":"<p>In very brief: the design approach is to achieve the multicast semantics of edge placement by two layers of activity.  Between the two layers sit mailbox workspaces: these exist in the center, and there is one for each edge cluster.  One layer of activity runs in the center and relates the edge placement problems to mailbox workspace contents.  The other layer is syncers, one in each edge cluster, that relate the corresponding mailbox contents with their local clusters.</p> <p>As in TMC, in this design we have downsync and upsync --- but they are a little more complicated here.  Downsync involves propagation of desired state from workload management workspace through mailbox workspaces to edge and return/summarization of reported state.  Upsync involves return/summarization of desired and reported state of objects born on the edge clusters.  On the inward path, the reported or full state goes from edge to the mailbox workspace and then is summarized to the workload management workspace.  State propagation is maintained in an eventually consistent way, it is not just one-and-done.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#inventory-management-workspaces","title":"Inventory Management workspaces","text":"<p>In this design the primary interface between infrastructure management and workload management is API objects in inventory management workspaces.  We abuse the <code>Location</code> and <code>SyncTarget</code> object types from kcp TMC for this purpose.  The people doing infrastructure management are responsible for creating the inventory management workspaces and populating them with <code>Location</code> and <code>SyncTarget</code> objects, one <code>Location</code> and one <code>SyncTarget</code> per edge cluster.  These inventory management workspaces need to use APIBindings to APIExports defining <code>Location</code> and <code>SyncTarget</code> so that the workload management layer can use one APIExport view for each API group to read those objects.</p> <p>To complete the plumbing of the syncers, each inventory workspace that contains a SyncTarget needs to also contain the following associated objects.  FYI, these are the things that <code>kubectl kcp workload sync</code> directly creates besides the SyncTarget.  Ensuring their presence is part of the problem of bootstrapping the workload management layer and is not among the things that this PoC takes a position on.</p> <ol> <li>A ServiceAccount that the syncer will authenticate as.</li> <li>A ClusterRole manipulating that SyncTarget and the    APIResourceImports (what are these?).</li> <li>A ClusterRoleBinding that links that ServiceAccount with that    ClusterRole.</li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edge-service-provider-workspace","title":"Edge Service Provider workspace","text":"<p>The edge multi-cluster service is provided by one workspace that includes the following things.</p> <ul> <li>An APIExport of the edge API group.</li> <li>The edge controllers: scheduler, placement translator, mailbox   controller, and status sumarizer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#workload-management-workspaces","title":"Workload Management workspaces","text":"<p>The users of edge multi-cluster primarily maintain these.  Each one of these has both control (API objects that direct the behavior of the edge computing platform) and data (API objects that hold workload desired and reported state).</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#data-objects","title":"Data objects","text":"<p>The workload desired state is represented by kube-style API objects, in the way that is usual in the Kubernetes milieu.  For edge computing we need to support both cluster-scoped (AKA non-namespaced) kinds as well as namespaced kinds of objects.</p> <p>The use of a workspace as a mere container presents a challenge, because some kinds of kubernetes API objects at not merely data but also modify the behavior of the apiserver holding them.  To resolve this dilemma, the edge users of such a workspace will use a special view of the workspace that holds only data objects.  The ones that modify apiserver behavior will be translated by the view into \"denatured\" versions of those objects in the actual workspace so that they have no effect on it.  And for these objects, the transport from center-to-edge will do the inverse: translate the denatured versions into the regular (\"natured\"?) versions for appearance in the edge cluster.  Furthermore, for some kinds of objects that modify apiserver behavior we want them \"natured\" at both center and edge.  There are thus a few categories of kinds of objects.  Following is a listing, with with the particular kinds that appear in kcp or plain kubernetes.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-denatured-in-center-natured-in-edge","title":"Needs to be denatured in center, natured in edge","text":"<p>For these kinds of objects: clients of the real workload management workspace can manipulate some such objects that will modify the behavior of the workspace, while clients of the edge computing view will manipulate distinct objects that have no effect on the behavior of the workspace.  These are kinds of objects to which kcp normally associates some behavior.  To be fully precise, the concern here is with behavior that is externally visible (including externally visible behavior of the server itself); we do not care to dissociate server-internal behavior such as storing encrypted at rest.  The edge computing platform will have to implement that view which dissociates the normal kcp behavior.</p> APIVERSION KIND NAMESPACED admissionregistration.k8s.io/v1 MutatingWebhookConfiguration false admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration false flowcontrol.apiserver.k8s.io/v1beta2 FlowSchema false flowcontrol.apiserver.k8s.io/v1beta2 PriorityLevelConfiguration false rbac.authorization.k8s.io/v1 ClusterRole false rbac.authorization.k8s.io/v1 ClusterRoleBinding false rbac.authorization.k8s.io/v1 Role true rbac.authorization.k8s.io/v1 RoleBinding true v1 LimitRange true v1 ResourceQuota true v1 ServiceAccount true"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-natured-in-center-and-edge","title":"Needs to be natured in center and edge","text":"<p>These should have their usual effect in both center and edge; they need no distinct treatment.</p> <p>Note, however, that they do have some sequencing implications.  They have to be created before any dependent objects, deleted after all dependent objects.</p> APIVERSION KIND NAMESPACED apiextensions.k8s.io/v1 CustomResourceDefinition false v1 Namespace false"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-natured-in-center-not-destined-for-edge","title":"Needs to be natured in center, not destined for edge","text":"APIVERSION KIND NAMESPACED apis.kcp.io/v1alpha1 APIBinding false <p>A workload management workspace needs an APIBinding to the APIExport of the API group <code>edge.kcp.io</code> from the edge service provider workspace, in order to be able to contain EdgePlacement and related objects.  These objects and that APIBinding are not destined for the edge clusters.</p> <p>The edge clusters are not presumed to be kcp workspaces, so APIBindings do not propagate to the edge clusters.  However, it is possible that APIBindings for workload APIs may exist in a workload management workspace and be selected for downsync to mailbox workspaces while the edge clusters have the same resources defined by CRDs (as mentioned later in the discussion of built-in resources and namespaces).</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#for-features-not-supported","title":"For features not supported","text":"<p>These are part of k8s or kcp APIs that are not supported by the edge computing platform.</p> APIVERSION KIND NAMESPACED apiregistration.k8s.io/v1 APIService false apiresource.kcp.io/v1alpha1 APIResourceImport false apiresource.kcp.io/v1alpha1 NegotiatedAPIResource false apis.kcp.io/v1alpha1 APIConversion false <p>The APIService objects are of two sorts: (a) those that are built-in and describe object types built into the apiserver and (b) those that are added by admins to add API groups served by custom external servers.  Sort (b) is not supported because this PoC does not support custom external servers in the edge clusters.  Sort (a) is not programmable in this PoC, but it might be inspectable.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#not-destined-for-edge","title":"Not destined for edge","text":"<p>These kinds of objects are concerned with either (a) TMC control or (b) workload data that should only exist in the edge clusters.  These will not be available in the view used by edge clients to maintain their workload desired and reported state.</p> APIVERSION KIND NAMESPACED apis.kcp.io/v1alpha1 APIExport false apis.kcp.io/v1alpha1 APIExportEndpointSlice false apis.kcp.io/v1alpha1 APIResourceSchema false apps/v1 ControllerRevision true authentication.k8s.io/v1 TokenReview false authorization.k8s.io/v1 LocalSubjectAccessReview true authorization.k8s.io/v1 SelfSubjectAccessReview false authorization.k8s.io/v1 SelfSubjectRulesReview false authorization.k8s.io/v1 SubjectAccessReview false certificates.k8s.io/v1 CertificateSigningRequest false coordination.k8s.io/v1 Lease true core.kcp.io/v1alpha1 LogicalCluster false core.kcp.io/v1alpha1 Shard false events.k8s.io/v1 Event true scheduling.kcp.io/v1alpha1 Location false scheduling.kcp.io/v1alpha1 Placement false tenancy.kcp.io/v1alpha1 ClusterWorkspace false tenancy.kcp.io/v1alpha1 Workspace false tenancy.kcp.io/v1alpha1 WorkspaceType false topology.kcp.io/v1alpha1 Partition false topology.kcp.io/v1alpha1 PartitionSet false v1 Binding true v1 ComponentStatus false v1 Event true v1 Node false workload.kcp.io/v1alpha1 SyncTarget false"},{"location":"Coding%20Milestones/PoC2023q1/outline/#already-denatured-in-center-want-natured-in-edge","title":"Already denatured in center, want natured in edge","text":"<p>These are kinds of objects that kcp already gives no interpretation to, and that is what edge-mc needs from the center workspaces.</p> <p>This is the default category of kind of object --- any kind of data object not specifically listed in another category is implicitly in this category.  Following are the kinds from k8s and kcp that fall in this category.</p> APIVERSION KIND NAMESPACED apps/v1 DaemonSet true apps/v1 Deployment true apps/v1 ReplicaSet true apps/v1 StatefulSet true autoscaling/v2 HorizontalPodAutoscaler true batch/v1 CronJob true batch/v1 Job true networking.k8s.io/v1 Ingress true networking.k8s.io/v1 IngressClass false networking.k8s.io/v1 NetworkPolicy true node.k8s.io/v1 RuntimeClass false policy/v1 PodDisruptionBudget true scheduling.k8s.io/v1 PriorityClass false storage.k8s.io/v1 CSIDriver false storage.k8s.io/v1 CSINode false storage.k8s.io/v1 CSIStorageCapacity true storage.k8s.io/v1 StorageClass false storage.k8s.io/v1 VolumeAttachment false v1 ConfigMap true v1 Endpoints true v1 PersistentVolume false v1 PersistentVolumeClaim true v1 Pod true v1 PodTemplate true v1 ReplicationController true v1 Secret true v1 Service true <p>Note that some <code>ConfigMap</code> and <code>Secret</code> objets are treated differently, as explained in the next section.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#system-infrastructure-objects","title":"System infrastructure objects","text":"<p>Even in a kcp workspace, some certain objects --- called here \"system infrastructure objects\" --- are created as a consequence of certain other objects or things.  The system infrastructure objects are tolerated in the center and do not propagate toward the edge.  Here is an initial list of system infrastructure objects:</p> <ul> <li><code>Secret</code> objects whose type is   <code>kubernetes.io/service-account-token</code> (these are automatically   created to support a <code>ServiceAccount</code> in some circumstances) or   <code>bootstrap.kubernetes.io/token</code>;</li> <li><code>ConfigMap</code> objects named <code>kube-root-ca.crt</code>;</li> <li><code>ServiceAcount</code> objects named <code>default</code> (these are automatically   created as a consequence of a namespace being created).</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#built-in-resources-and-namespaces","title":"Built-in resources and namespaces","text":"<p>An edge cluster has some built-in resources (i.e, kinds of objects) and namespaces.  A resource may be built-in by any of several ways: it can be built-in to the apiserver, it can be defined by a CRD, its API group can be delegated by an APIService to a custom external server (each of the latter two is sometimes called \"aggregation\").  Note also that a resource may be defined in edge clusters one way (e.g., by being built into kube-apiserver) and in the workload management workspace another way (e.g., by a CustomResourceDefinition).</p> <p>In this PoC, all edge clusters are considered to have the same built-in resources and namespaces.</p> <p>As a matter of scoping the work here, it is also assumed that each API group built into the edge clusters supports the API versions chosen by the conflict resolution rules below when they are applied to the workload sources.</p> <p>At deployment time the workload management platform is configured with lists of resources and namespaces built into the edge clusters.</p> <p>Propagation from center to edge does not attempt to manage the resource and namespace definitions that are built into the edge clusters.</p> <p>The mailbox workspaces will have built-in resources and namespaces that are a subset of those built into the edge clusters.  The propagation from workload management workspace to mailbox workspace does not attempt to manage the resource and namespace definitions that are built into the mailbox workspaces.</p> <p>The above wording is deliberately restrained, for the sake of flexibility regarding resources that are defined one way in the edge clusters and another way in workload management workspace.  For example, the following scenario is allowed.</p> <ul> <li>Some central team owns an API group and produces some   CustomResourceDefinition (CRD) objects that populate that API group.</li> <li>That team derives APIResourceSchemas from those CRDs and a   corresponding APIExport of their API group.</li> <li>That team maintains a kcp workspace holding those APIResourceSchemas   and that APIExport.</li> <li>Some workload management workspaces have APIBindings to that   APIExport, and EdgePlacement objects that (1) select those   APIBinding objects for downsync and (2) select objects of kinds   defined through those APIBindings for either downsync or upsync.</li> <li>Those resources are built into the edge clusters by pre-deploying   the aforementioned CRDs there.</li> <li>Those resources are not built into the mailbox workspaces.  In   this case the APIBindings would propagate from workload management   workspace to mailbox workspaces but not edge clusters.</li> <li>As a consequence of those propagated APIBindings, the APIExport's   view includes all of the objects (in workload management workspaces,   in mailbox workspaces, and in any other workspaces where they   appear) whose kind is defined through those APIBindings.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#control-objects","title":"Control objects","text":"<p>These are the EdgePlacement objects, their associated SinglePlacementSlice objects, and the objects that direct customization and summarization.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edgeplacement-objects","title":"EdgePlacement objects","text":"<p>One of these is a binding between a \"what\" predicate and a \"where\" predicate.</p> <p>Overlaps between EdgePlacement objects are explicitly allowed.  Two EdgePlacement objects may have \"where\" predicates that both match some of the same destinations.  Two EdgePlacement objects may have \"what\" predicates that match some of the same workload descriptions.  Two EdgePlacement objects may overlap in both ways.</p> <p>An EdgePlacement object deliberately only binds \"what\" and \"where\", without any adverbs (such as prescriptions of customization or summarization).  This means that overlapping EdgePlacement objects can not conflict in those adverbs.</p> <p>However, another sort of conflict remains possible.  This is because the user controls the IDs --- that is, the names --- of the parts of the workload.  In full, a Kubernetes API object is identified by API group, API major version, Kind (equivalently, resource name), namespace if relevant, and name.  For simplicity in this PoC we will not worry about differences in API major version; each API group in Kubernetes and/or kcp currently has only one major version.</p> <p>Two different workload descriptions can have objects with the same ID (i.e., if they appear in different workspaces).  These objects, when rendered to the same API version, might have different values.  And the objects may be available in different API versions in different source workspaces.  See client-go for what an API server says about which versions it can serve for a given API group, and meta/v1 for the supporting details on an APIGroup struct.</p> <p>When multiple workload objects with the same APIGroup, Kind, namespace (if namespaced), and name are directed to the same edge cluster, they are merged with conflicts handled by (a) a rule for resolution and (b) reporting via both error logging and Kubernetes Event creation.  These conflicts are serious matters: they mean user expectations are not being met (because they are inconsistent); this is why the placement translator tries hard to make the user aware.</p> <p>The first part of merging a set of objects is to read them all at the same API version.  The placement translator solves the problem of picking API version at the level of API groups rather than object-by-object.  The API version for an given API group is chosen as follows.  First, take the intersection of the supported versions from the various sources.  If this intersection is empty then this is a conflict.  It is resolved by throwing out the APIGroup with the lowest version and repeating with the reduced set of APIGroup structs. Next, take the union of the preferred versions. If this union has a non-empty intersection with the intersection of the supported versions, take the following steps with this intersection; otherwise proceed with just the intersection of the supported versions.  When first (since process startup) presented with an instance of this problem, the placement translator picks the highest version from this intersection.  Subsequently for the same API group, the placement translator sticks with its previous decision as long as that is still in the intersection.  If the previous choice is no longer avaiable, the highest version is picked.  This preference for highest version is based on the expectation that rolling forward will be more common than rolling back; using the intersection ensures that both work (as long as the collection of sources has an overlap in supported versions, which is basic sanity).</p> <p>A workload prescription object that is in the process of graceful deletion (i.e., with <code>DeletionTimestamp</code> set to something) is considered here to already be gone.</p> <p>Once they have been read at a consistent API version, merging of multiple objects is done as follows.  Different parts of the object are handled differently.</p> <ul> <li>TypeMeta.  This can not conflict because it is part of what   identifies an object.</li> <li>ObjectMeta.</li> <li>Labels and Annotations.  These are handled on a key-by-key     basis.  Distinct keys do not conflict.  When multiple objects have     a label or annotation with the same key, the corresponding value     in the result is the value from the most recently updated of those     objects.</li> <li>OwnerReferences.  This is handled analogously to labels and     annotations.  The key is the combination of APIVersion, Kind, and     Name.</li> <li>Finalizers.  This is simply a set of strings.  The result of     merging is the union of the sets.</li> <li>ManagedFields.  This is metadata that is not propagated.</li> <li>Spec.  Beyond TypeMeta and ObjectMeta, the remaining object   fields are specific to the kind of object.  Many have a field named   \"Spec\" in the Go language source, \"spec\" in the JSON representation.   For objects that have Spec fields, merging has a conflict if those   field values are not all equal when considered as JSON data, and the   resolution is to take the value from the most recently updated   object.</li> <li>Status.  Status is handled analogously to Spec.  For both, we   consider a missing field to be the same as a field with a value of   <code>nil</code>.  That is expected to be the common case for the Status of   these workload prescription objects.</li> <li>Other fields.  If all the values are maps (objects in   JavaScript) then they are merged key-by-key, as for labels and   annotations.  Otherwise they are treated as monoliths, as for Spec   and Status.</li> </ul> <p>For the above, the most recently updated object is determined by parsing the ResourceVersion as an <code>int64</code> and picking the highest value.  This is meaningful under the assumption that all the source workspaces are from the same kcp server --- which will be true for this PoC but is not a resaonble assumption in general.  Also: interpreting ResourceVersion breaks a rule for Kubernetes clients --- but this is dismayingly common.  Beyond this PoC we could hope to do better by looking at the ManagedFields.  But currently kcp does not include https://github.com/kubernetes/kubernetes/pull/110058 so the ManagedFields often do not properly reflect the update times.  Even so, those timestamps have only 1-second precision --- so conflicts will remain possible (although, hopefully, unlikely).</p> <p>There is special handling for Namespace objects.  When a workload includes namespaced objects, the propagation has to include ensuring that the corresponding Namespace object exists in the destination.  An EdgePlacement's \"what\" predicate MAY fail to match a relevant Namespace object.  This is taken to mean that this EdgePlacement is not requesting propagation of the details (Spec, labels, etc.) of that Namespace object but only expects propagation to somehow ensure that the namespace exists.  When merging overlapping workloads that have namespaces in common, only the Namespace objects that come from matching a \"what\" predicate need to be merged.</p> <p>The above also provide an answer to the question of what version is used when writing to the mailbox workspace and edge cluster.  The version used for that is the version chosen above.  In the case of no conflicts, this means that the writes are done using the preferred version from the API group from the workload management workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#mailbox-workspaces","title":"Mailbox workspaces","text":"<p>The mailbox controller maintains one mailbox workspace for each SyncTarget.  A mailbox workspace acts as a workload source for the corresponding syncer, prescribing the workload to go to the corresponding edge cluster and the <code>SyncerConfig</code> object that guides the syncer.</p> <p>A mailbox workspace contains the following items.</p> <ol> <li>APIBindings (maintained by the mailbox controller) to APIExports of    workload object types.</li> <li>Workload objects, post customization in the case of downsynced    objects.</li> <li>A <code>SyncerConfig</code> object.</li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edge-cluster","title":"Edge cluster","text":"<p>Also called edge pcluster.</p> <p>One of these contains the following items.  FYI, these are the things in the YAML output by <code>kubectl kcp workload edge-sync</code>.  The responsibility for creating and maintaining these objects is part of the problem of bootstrapping the workload management layer and is not among the things that this PoC takes a position on.</p> <ul> <li>A namespace that holds the syncer and associated objects.</li> <li>A ServiceAccount that the syncer authenticates as when accessing the   views of the center and when accessing the edge cluster.</li> <li>A Secret holding that ServiceAccount's authorization token.</li> <li>A ClusterRole listing the non-namespaced privileges that the   syncer will use in the edge cluster.</li> <li>A ClusterRoleBinding linking the syncer's ServiceAccount and ClusterRole.</li> <li>A Role listing the namespaced privileges that the syncer will use in   the edge cluster.</li> <li>A RoleBinding linking the syncer's ServiceAccount and Role.</li> <li>A Secret holding the kubeconfig that the syncer will use to access   the edge cluster.</li> <li>A Deployment of the syncer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#mailbox-controller","title":"Mailbox Controller","text":"<p>This controller maintains one mailbox workspace per SyncTarget.  Each of these mailbox workspaces is used for a distinct syncing problem: downsynced objects go here from their workload management workspaces, and upsynced objects go here from the edge cluster.  These workspaces are all children of the edge service provider workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#kubestellar-scheduler","title":"KubeStellar Scheduler","text":"<p>This controller monitors the EdgePlacement, Location, and SyncTarget objects and maintains the results of matching.  For each EdgePlacement object this controller maintains an associated collection of SinglePlacementSlice objects holding the matches for that EdgePlacement.  These SinglePlacementSlice objects appear in the same workspace as the corresponding EdgePlacement; the remainder of how they are linked is TBD.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#placement-translator","title":"Placement Translator","text":"<p>This controller continually monitors all the EdgePlacement objects, SinglePlacementSlice objects, and related workload objects, and maintains the proper projections of those into mailbox workspace contents.  The customization, if any, is done in this process.  Note also that everything that has to be denatured in the workload management workspace also has to be denatured in the mailbox workspace.</p> <p>The job of the placement translator can be broken down into the following five parts.</p> <ul> <li>Resolve each EdgePlacement's \"what\" part to a list of particular   workspace items (namespaces and non-namespaced objects).</li> <li>Monitor the SinglePlacementSlice objects that report the scheduler's   resolutions of the \"where\" part of the EdgePlacement objects.</li> <li>Maintain the association between the resolved \"where\" from the   scheduler and the resolved what.</li> <li>Maintain the copies, with customization, of the workload objects   from source workspace to mailbox workspaces.</li> <li>Maintain the SyncerConfig object in each mailbox workspace to direct   the corresponding syncer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#syncers","title":"Syncers","text":"<p>In this PoC there is a 1:1:1 relation between edge cluster, mailbox workspace, and syncer.  The syncer runs in the edge cluster and does downsync from and upsync to the mailbox workspace.  The syncer monitors a SyncerConfig object in the mailbox workspace to know what to downsync and upsync.</p> <p>For those familiar with kcp's TMC syncer, note that the edge syncer differs in the following ways.</p> <ul> <li>Create self-sufficient edge clusters.</li> <li>Re-nature objects that edge-mc forcibly denatures at the center.</li> <li>Return reported state from associated objects.</li> <li>Does not access the SyncTarget object.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#status-summarizer","title":"Status Summarizer","text":"<p>For each EdgePlacement object and related objects this controller maintains the directed status summary objects.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#usage-scenario","title":"Usage Scenario","text":"<p>The usage scenario breaks, at the highest level, into two parts: inventory and workload.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#inventory-usage","title":"Inventory Usage","text":"<p>A user with infrastructure authority creates one or more inventory management workspaces.  Each such workspace needs to have the following items, which that user will create if they are not pre-populated by the workspace type.</p> <ul> <li>An APIBinding to the <code>workload.kcp.io</code> APIExport to get   <code>SyncTarget</code>.</li> <li>An APIBinding to the <code>scheduling.kcp.io</code> APIExport to get   <code>Location</code>.</li> <li>A ServiceAccount (with associated token-bearing Secret) (details   TBD) that the mailbox controller authenticates as.</li> <li>A ClusterRole and ClusterRoleBinding that authorize said   ServiceAccount to do what the mailbox controller needs to do.</li> </ul> <p>This user also creates one or more edge clusters.</p> <p>For each of those edge clusters, this user creates the following.</p> <ul> <li>a corresponding SyncTarget, in one of those inventory management   workspaces;</li> <li>a Location, in the same workspace, that matches only that   SyncTarget.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#workload-usage","title":"Workload usage","text":"<p>A user with workload authority starts by creating one or more workload management workspaces.  Each needs to have the following, which that user creates if the workload type did not already provide.</p> <ul> <li>An APIBinding to the APIExport of <code>edge.kcp.io</code> from the edge   service provider workspace.</li> <li>For each of the Scheduler, the Placement Translator, and the   Status Summarizer:</li> <li>A ServiceAccount for that controller to authenticate as;</li> <li>A ClusterRole granting the privileges needed by that controller;</li> <li>A ClusterRoleBinding that binds those two.</li> </ul> <p>This user also uses the edge-workspace-as-container view of each such workspace to describe the workload desired state.</p> <p>This user creates one or more EdgePlacement objects to say which workload goes where.  These may be accompanied by API objects that specify rule-baesd customization, specify how status is to be summarized.</p> <p>The edge-mc implementation propagates the desired state from center to edge and collects the specified information from edge to center.</p> <p>The edge user monitors status summary objects in their workload management workspaces.</p> <p>The status summaries may include limited-length lists of broken objects.</p> <p>Full status from the edge is available in the mailbox workspaces.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/","title":"Placement Translator","text":"<p>The placement translator runs in the center and translates EMC placement problems into edge sync problems.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#status","title":"Status","text":"<p>The placement translator is a work in progress.  It maintains <code>SyncerConfig</code> objects and downsynced objects in mailbox workspaces, albeit with limitations discussed in the next section.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#additional-design-details","title":"Additional Design Details","text":"<p>The placement translator maintains one <code>SyncerConfig</code> object in each mailbox workspace.  That object is named <code>the-one</code>.  Other <code>SyncerConfig</code> objects may exist; the placement translator ignores them.</p> <p>The placement translator responds to each resource discovery independently.  This makes the behavior jaggy and the logging noisy. For example, it means that the <code>SyncerConfig</code> objects may be rewritten for each resource discovery.  But eventually the right things happen.</p> <p>The placement translator does not yet attempt the full prescribed technique for picking the API version to use when reading and writing. Currently it looks only at the preferred version reported in each workload management workspace, and only succeeds if they all agree.</p> <p>One detail left vague in the design outline is what constitutes the \"desired state\" that propagates from center to edge.  The easy obvious answer is the \"spec\" section of downsynced objects, but that answer ignores some issues.  Following is the current full answer.</p> <p>When creating a workload object in a mailbox workspace, the placement translator uses a copy of the object read from the workload management workspace but with the following changes.</p> <ul> <li>The <code>metadata.managedFields</code> is emptied.</li> <li>The <code>metadata.resourceVersion</code> is emptied.</li> <li>The <code>metadata.selfLlink</code> is emptied.</li> <li>The <code>metadata.uid</code> is emptied.</li> <li>The <code>metadata.ownerReferences</code> is emptied.  (Doing better would   require tracking UID mappings from WMW to MBWS.)</li> <li>In <code>metadata.labels</code>, <code>edge.kcp.io/projected=yes</code> is added.</li> </ul> <p>The placement translator does not react to changes to the workload objects in the mailbox workspace.</p> <p>When downsyncing desired state and the placement translator finds the object already exists in the mailbox workspace, the placement translator does an HTTP PUT (<code>Update</code> in the <code>k8s.io/client-go/dynamic</code> package) using an object value --- called below the \"destination\" object --- constructed by reading the object from the MBWS and making the following changes.</p> <ul> <li>For top-level sections in the source object other than <code>apiVersion</code>,   <code>kind</code>, <code>metadata</code>, and <code>status</code>, the destination object gets the   same contents for that section.</li> <li>If the source object has some annotations then they are merged into   the destination object annotations as follows.</li> <li>A destination annotation that has no corresponding annotation in     the source is unchanged.</li> <li>A destination annotation that has the same value as the     corresponding annotation in the source is unchanged.</li> <li>A \"system\" annnotation is unchanged.  The system annotations are     those whose key (a) starts with <code>kcp.io/</code> or other stuff followed     by <code>.kcp.io/</code> and (b) does not start with <code>edge.kcp.io/</code>.</li> <li>The source object's labels are merged into the destination object   using the same rules as for annotations, and <code>edge.kcp.io/projected</code>   is set to <code>yes</code>.</li> <li>The remainder of the <code>metadata</code> is unchanged.</li> </ul> <p>For objects --- other than <code>Namespace</code> objects --- that exist in a mailbox workspace and whose API GroupResource has been relevant to the placement translator since it started, ones that have the <code>edge.kcp.io/projected=yes</code> label but are not currently desired are deleted.  The exclusion for <code>Namespace</code> objects is there because the placement translator does not take full ownership of them, rather it takes the position that there might be other parties that create <code>Namespace</code> objects or rely on their existence.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#usage","title":"Usage","text":"<p>The placement translator needs three kube client configurations.  One points to the edge service provider workspace and provides authority to write into the mailbox workspaces.  Another points to the kcp server base (i.e., does not identify a particular logical cluster nor <code>*</code>) and is authorized to read all clusters.  In the kubeconfig created by <code>kcp start</code> that is satisfied by the context named <code>system:admin</code>.  The third points to the \"scheduling service provider workspace\", the one that has the APIExport of <code>scheduling.kcp.io</code>. This is normally <code>root</code>, which has all the kcp APIExports, and the context named <code>root</code> in the kubeconfig created by <code>kcp start</code> satisfies this.</p> <p>The command line flags, beyond the basics, are as follows.  For a string parameter, if no default is explicitly stated then the default is the empty string, which usually means \"not specified here\".  For both kube client configurations, the usual rules apply: first consider command line parameters, then <code>$KUBECONFIG</code>, then <code>~/.kube/config</code>.</p> <pre><code>      --allclusters-cluster string       The name of the kubeconfig cluster to use for access to all clusters\n      --allclusters-context string       The name of the kubeconfig context to use for access to all clusters (default \"system:admin\")\n      --allclusters-kubeconfig string    Path to the kubeconfig file to use for access to all clusters\n      --allclusters-user string          The name of the kubeconfig user to use for access to all clusters\n      --espw-cluster string              The name of the kubeconfig cluster to use for access to the edge service provider workspace\n      --espw-context string              The name of the kubeconfig context to use for access to the edge service provider workspace\n      --espw-kubeconfig string           Path to the kubeconfig file to use for access to the edge service provider workspace\n      --espw-user string                 The name of the kubeconfig user to use for access to the edge service provider workspace\n      --server-bind-address ipport       The IP address with port at which to serve /metrics and /debug/pprof/ (default :10204)\n      --sspw-cluster string              The name of the kubeconfig cluster to use for access to the scheduling service provider workspace\n      --sspw-context string              The name of the kubeconfig context to use for access to the scheduling service provider workspace (default \"root\")\n      --sspw-kubeconfig string           Path to the kubeconfig file to use for access to the scheduling service provider workspace\n      --sspw-user string                 The name of the kubeconfig user to use for access to the scheduling service provider workspace\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#try-it","title":"Try It","text":"<p>The nascent placement translator can be exercised following the scenario in example1.  You will need to run the scheduler and mailbox controller long enough for them to create what this scenario calls for, but they can be terminated after that.</p> <p>When you get to the step of \"Populate the edge service provider workspace\", it suffices to do the following.</p> <pre><code>$ kubectl ws root:espw\n$ kubectl create -f config/exports\n</code></pre> <p>Continue to follow the steps until the start of Stage 3 of the exercise.</p> <p>Next make sure you run <code>kubectl ws root:espw</code> to enter the edge service provider workspace, then you will be ready to run the edge controllers.</p> <p>First run the mailbox controller, long for it to create the mailbox workspaces; it does not need to keep running, you can ^C it.  This should not take long, and do not expect an explicit ackowledgement on the console.  You can check for their existence by doing <code>kubectl get workspaces</code> while in the ESPW.</p> <p>Next run the scheduler, long enough for it to create the SinglePlacementSlice objects.  Again, this should not take long, and you can ^C the scheduler once it has created those objects.</p> <p>Finally run the placement translator from the command line.  That should look like the following (possibly including some complaints, which do not necessarily indicate real problems because the subsequent success is not logged so profligately).</p> <pre><code>$ go run ./cmd/placement-translator\n\nI0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\nI0412 15:15:57.970014   94634 shared_informer.go:289] Caches are synced for what-resolver\nI0412 15:15:57.970178   94634 shared_informer.go:282] Waiting for caches to sync for where-resolver\nI0412 15:15:57.970192   94634 shared_informer.go:289] Caches are synced for where-resolver\n...\nI0412 15:15:57.972185   94634 map-types.go:338] \"Put\" map=\"where\" key=\"r0bdh9oumjkoag3s:edge-placement-s\" val=\"[&amp;{SinglePlacementSlice edge.kcp.io/v1alpha1} {edge-placement-s    e1b1033d-49f2-45e8-8a90-6d0295b644b6 1184 1 2023-04-12 14:39:21 -0400 EDT &lt;nil&gt; &lt;nil&gt; map[] map[kcp.io/cluster:r0bdh9oumjkoag3s] [{edge.kcp.io/v1alpha1 EdgePlacement edge-placement-s 0e718a31-db21-47f1-b789-cd55835b1418 &lt;nil&gt; &lt;nil&gt;}] []  [{scheduler Update edge.kcp.io/v1alpha1 2023-04-12 14:39:21 -0400 EDT FieldsV1 {\\\"f:destinations\\\":{},\\\"f:metadata\\\":{\\\"f:ownerReferences\\\":{\\\".\\\":{},\\\"k:{\\\\\\\"uid\\\\\\\":\\\\\\\"0e718a31-db21-47f1-b789-cd55835b1418\\\\\\\"}\\\":{}}}} }]} [{1xpg93182scl85te location-g sync-target-g 5ee1c42e-a7d5-4363-ba10-2f13fe578e19}]}]\"\nI0412 15:15:57.973740   94634 map-types.go:338] \"Put\" map=\"where\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val=\"[&amp;{SinglePlacementSlice edge.kcp.io/v1alpha1} {edge-placement-c    c446ca9b-8937-4751-89ab-058bcfb079c1 1183 3 2023-04-12 14:39:21 -0400 EDT &lt;nil&gt; &lt;nil&gt; map[] map[kcp.io/cluster:1i1weo8uoea04wxr] [{edge.kcp.io/v1alpha1 EdgePlacement edge-placement-c c1e038b9-8bd8-4d22-8ab8-916e40c794d1 &lt;nil&gt; &lt;nil&gt;}] []  [{scheduler Update edge.kcp.io/v1alpha1 2023-04-12 14:39:21 -0400 EDT FieldsV1 {\\\"f:destinations\\\":{},\\\"f:metadata\\\":{\\\"f:ownerReferences\\\":{\\\".\\\":{},\\\"k:{\\\\\\\"uid\\\\\\\":\\\\\\\"c1e038b9-8bd8-4d22-8ab8-916e40c794d1\\\\\\\"}\\\":{}}}} }]} [{1xpg93182scl85te location-f sync-target-f e6efb8bd-6755-45ac-b44d-5d38f978f990} {1xpg93182scl85te location-g sync-target-g 5ee1c42e-a7d5-4363-ba10-2f13fe578e19}]}]\"\n...\nI0412 15:15:58.173974   94634 map-types.go:338] \"Put\" map=\"what\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val={Downsync:map[{APIGroup: Resource:namespaces Name:commonstuff}:{APIVersion:v1 IncludeNamespaceObject:false}] Upsync:[{APIGroup:greoup1.test Resources:[sprockets flanges] Namespaces:[orbital] Names:[george cosmo]} {APIGroup:group2.test Resources:[cogs] Namespaces:[] Names:[William]}]}\nI0412 15:15:58.180380   94634 map-types.go:338] \"Put\" map=\"what\" key=\"r0bdh9oumjkoag3s:edge-placement-s\" val={Downsync:map[{APIGroup: Resource:namespaces Name:specialstuff}:{APIVersion:v1 IncludeNamespaceObject:false}] Upsync:[{APIGroup:greoup1.test Resources:[sprockets flanges] Namespaces:[orbital] Names:[george cosmo]} {APIGroup:group3.test Resources:[widgets] Namespaces:[] Names:[*]}]}\n...\n</code></pre> <p>The \"Put\" log entries with <code>map=\"what\"</code> show what the \"what resolver\" is reporting.  This reports mappings from <code>ExternalName</code> of an <code>EdgePlacement</code> object to the workload parts that that <code>EdgePlacement</code> says to downsync and upsync.</p> <p>The \"Put\" log entries with <code>map=\"where\"</code> show the <code>SinglePlacementSlice</code> objects associated with each <code>EdgePlacement</code>.</p> <p>Next, using a separate shell, examine the SyncerConfig objects in the mailbox workspaces.  Make sure to use the same kubeconfig as you use to run the placement translator, or any other that is pointed at the edge service provider workspace. The following with switch the focus to mailbox workspace(s).</p> <p>You can get a listing of mailbox workspaces, while in the edge service provider workspace, as follows.</p> <pre><code>$ kubectl get Workspace\nNAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19   universal            Ready   https://192.168.58.123:6443/clusters/12zzf3frkqz2yj39   36m\n1xpg93182scl85te-mb-e6efb8bd-6755-45ac-b44d-5d38f978f990   universal            Ready   https://192.168.58.123:6443/clusters/2v6wl3x41zxmpmhr   36m\n</code></pre> <p>Next switch to one of the mailbox workspaces (in my case I picked the one for the guilder cluster) and examine the <code>SyncerConfig</code> object. That should look like the following.</p> <pre><code>$ kubectl ws 1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\nCurrent workspace is \"root:espw:1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\" (type root:universal).\n$ kubectl get SyncerConfig the-one -o yaml                           apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12zzf3frkqz2yj39\n  creationTimestamp: \"2023-04-12T19:15:58Z\"\n  generation: 2\n  name: the-one\n  resourceVersion: \"1249\"\n  uid: 00bf8d10-393a-4d94-b032-79fae30646f6\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\n      resource: limitranges\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: \"\"\n      resource: resourcequotas\n    - apiVersion: v1\n      group: \"\"\n      resource: configmaps\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: events.k8s.io\n      resource: events\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\n      resource: events\n    - apiVersion: v1\n      group: \"\"\n      resource: secrets\n    - apiVersion: v1\n      group: \"\"\n      resource: services\n    - apiVersion: v1\n      group: \"\"\n      resource: pods\n    - apiVersion: v1\n      group: \"\"\n      resource: serviceaccounts\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n  upsync:\n  - apiGroup: group2.test\n    names:\n    - William\n    resources:\n    - cogs\n  - apiGroup: group3.test\n    names:\n    - '*'\n    resources:\n    - widgets\n  - apiGroup: greoup1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\nstatus: {}\n</code></pre> <p>At this point you might veer off from the example sceario and try tweaking things.  For example, try deleting an EdgePlacement as follows.</p> <pre><code>$ kubectl ws root:work-c\nCurrent workspace is \"root:work-c\".\n$ kubectl delete EdgePlacement edge-placement-c\nedgeplacement.edge.kcp.io \"edge-placement-c\" deleted\n</code></pre> <p>That will cause the placement translator to log updates, as follows.</p> <pre><code>I0412 15:20:43.129842   94634 map-types.go:338] \"Put\" map=\"what\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val={Downsync:map[] Upsync:[]}\nI0412 15:20:43.241674   94634 map-types.go:342] \"Delete\" map=\"where\" key=\"1i1weo8uoea04wxr:edge-placement-c\"\n</code></pre> <p>After that, the SyncerConfig in the florin mailbox should be empty, as in the following (you mailbox workspace names may be different).</p> <pre><code>$ kubectl ws root:espw\nCurrent workspace is \"root:espw\".\n$ kubectl ws 2lplrryirmv4xug3-mb-89c08764-01ae-4117-8fb0-6b752e76bc2f\nCurrent workspace is \"root:espw:2lplrryirmv4xug3-mb-89c08764-01ae-4117-8fb0-6b752e76bc2f\" (type root:universal).\n$ kubectl get SyncerConfig the-one -o yaml\napiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 2cow9p3xogak4n0u\n  creationTimestamp: \"2023-04-11T04:34:22Z\"\n  generation: 4\n  name: the-one\n  resourceVersion: \"2130\"\n  uid: 2b66b4bc-4130-4bf0-8524-73d6885f2ad8\nspec:\n  namespaceScope: {}\nstatus: {}\n</code></pre> <p>And the SyncerConfig in the guilder mailbox workspace should reflect only the special workload.  That would look something like the following.</p> <pre><code>$ kubectl ws root:espw\n\n$ kubectl ws 1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\nCurrent workspace is \"root:espw:1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\" (type root:universal).\n$ kubectl get SyncerConfig the-one -o yaml                           apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12zzf3frkqz2yj39\n  creationTimestamp: \"2023-04-12T19:15:58Z\"\n  generation: 3\n  name: the-one\n  resourceVersion: \"1254\"\n  uid: 00bf8d10-393a-4d94-b032-79fae30646f6\nspec:\n  namespaceScope:\n    namespaces:\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\n      resource: pods\n    - apiVersion: v1\n      group: \"\"\n      resource: events\n    - apiVersion: v1\n      group: \"\"\n      resource: limitranges\n    - apiVersion: v1\n      group: \"\"\n      resource: services\n    - apiVersion: v1\n      group: \"\"\n      resource: configmaps\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\n      resource: serviceaccounts\n    - apiVersion: v1\n      group: \"\"\n      resource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\n      resource: resourcequotas\n    - apiVersion: v1\n      group: events.k8s.io\n      resource: events\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\n    resources:\n    - widgets\n  - apiGroup: greoup1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\nstatus: {}\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/","title":"Possible Roadmaps for Particular Use Cases","text":"","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#background","title":"Background","text":"<p>The outline mentions features that need not be implement at first.  In the following sections we consider some particular use cases.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#mvi","title":"MVI","text":"<p>MVI needs customization.  We can demo an MVI scenario without: self-sufficient edge clusters, summarization, upsync (Return and/or summarization of reported state from associated objects), sophisticated handling of workload conflicts.</p> <p>What about denaturing?</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#compliance-to-policy","title":"Compliance-to-Policy","text":"<p>I am not sure what is workable here.  Following are some possibilities.  They vary in two dimensions.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#how-c2p-controller-consumes-reports","title":"How C2P controller consumes reports","text":"","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-view-of-workload-apiexport-and-workload-apibindings","title":"Through view of workload APIExport and workload APIBindings","text":"<p>As outlined in PR 241: - the C2P team maintains CRDs, APIResourceSchemas, and an APIExport for   the policy and report resources; - the C2P team puts those APIResourceSchemas and that APIExport in a   kcp workspace of their choice; - the workload management workspace has an APIBinding to that APIExport; - the EdgePlacement selects that APIBinding for downsync; - the APIBinding goes to the mailbox workspace but not the edge cluster; - those CRDs are pre-installed on the edge clusters; - the APIExport's view shows the report objects in the mailbox   workspaces (as well as anywhere else they exist).</p> <p>This is not a great choice because of \"those CRDs are pre-installed on the edge clusters\".</p> <p>It is also not a great choice because it requires the C2P team to maintain two copies of the Kyverno resource definitions.</p> <p>This is a bad choice because it is not consistent with the preferred way to demonstrate installation of Kyverno, which is to have Helm install Kyverno into the workload managemet workspace.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-a-new-kind-of-view","title":"Through a new kind of view","text":"<p>We could define a new kind of view that does what we want.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-view-of-emc-apiexport-and-mailbox-apibindings","title":"Through view of EMC APIExport and mailbox APIBindings","text":"<p>This approach also uses APIExport and APIBinding objects but in a different way than above.  In this approach the placement translator maintains one APIExport in the edge service provider workspace and a corresponding APIBinding object in each mailbox workspace, and they work together as follows.</p> <p>The APIExport has an empty LatestResourceSchemas but a large dynamic PermissionClaims slice.  In particular, there is a PermissionClaim for every resource involved in downsync or upsync in any EdgePlacement object.  Some day we might try something more granular, but today is not that day.</p> <p>In each mailbox workspace, the corresponding APIBinding's list of accepted PermissionClaims has an etry for every resource downsynced or upsynced to that workspace.</p> <p>As a consequence, the APIExport's view holds all the objects whose kind/resource is defined by those APIBindings.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-an-api-for-consuming-from-mailboxes","title":"Through an API for consuming from mailboxes","text":"<p>The C2P Controller uses the API proposed in PR 240 to read the report objects from the mailbox workspaces.  This has the downside of exposing the mailbox workspaces as part of the edge-mc interface --- which they were NOT originally intended to be.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#c2p-controller-consumes-report-summaries-prepared-by-edge-mc","title":"C2P Controller consumes report summaries prepared by edge-mc","text":"<p>In this scenario: - we have defined and implemented summarization in edge-mc; - that summarization is adequate for the needs of the C2P Controller; - that controller consumes summaries rather than the reports themselves.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#mailbox-vs-edge","title":"Mailbox vs. Edge","text":"<p>The latest plan is to use full EMC.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#using-the-current-tmc-syncer","title":"Using the current TMC syncer","text":"<p>In this scenario the edge clusters are not self-sufficient; the workload containers in an edge cluster use kube api services from the corresponding mailbox workspace.  The key insight here is that from an outside perspective, a pair of (edge cluster, corresponding mailbox workspace) operates as a unit and the rest of the world does not care about internal details of that unit.  But that is only true if you do not require too much from the networking.  In this scenario, a workload container runs in the edge cluster and a workload Service object is about proxying/load-balancing in the edge cluster.  An admission control webhook normally directs the apiserver to call out to a virtual IP address associated with a Service; that is a problem in this scenario because the apiserver in question is the one holding the mailbox workspace but the Service that gets connections to the workload containers is in the edge cluster.  This scenario will work if the C2P workload does not include admission control webhooks.  Note that Kubernetes release 1.26 introduces CEL-based validating admission control policies, so using them would not involve webhooks.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#expand-tmc-to-support-webhooks","title":"Expand TMC to support webhooks","text":"<p>The problem with webhooks would go away if TMC were expanded to support them, perhaps through some sort of tunneling so that a client in the center can open connections to a Service at the edge.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#pre-deploy-controllers-and-resources-on-edge-clusters","title":"Pre-deploy controllers and resources on edge clusters","text":"<p>In this scenario, the PVP/PEP is pre-deployed on the edge clusters, and the policy and report resources (which are cluster-scoped) are predefined there too.  This scenario would continue to use the TMC syncer, but only need it to downsync the policies and upsync the reports.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#use-full-emc","title":"Use full EMC","text":"<p>No shortcuts here, no limitations.</p>","tags":["code","milestone","poc2023q1"]},{"location":"Coding%20Milestones/PoC2023q1/environments/_index/","title":"Environments","text":"<p>There will be 2 environments designed, created, and maintained for KubeStellar: - A cloud environment (cloud-env) which will be used to deploy, scale, and measure metrics associated with small, medium, and large-scale KubeStellar experiments, - A development environment (dev-env) which will be used to deploy a small local KubeStellar installation that is sized for daily use and experimentation on a laptop.</p>"},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/","title":"Cloud-Environment (cloud-env)","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/#monitoring-tools-for-kubestellar-prometheus-grafana-and-node-exporter","title":"Monitoring Tools for KubeStellar (Prometheus, Grafana and Node Exporter)","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/#description","title":"Description","text":"<p>This example shows how to deploy monitoring tools (prometheus, grafana and node exporter) for KubeStellar components (core and edge regions) - see architecture image above. Prometheus server is deployed in the core region running the KCP server alongside the components for KubeStellar. A Prometheus agent is deployed in the edge regions running the edge pclusters</p> <ol> <li>Create your hosts file with the list of target hosts (KCP server &amp; Edge pclusters)</li> </ol> <pre><code>[kcp-server]\n192.168.56.2\n\n[edge-pclusters]\n192.160.56.10\n192.160.56.12\n</code></pre> <ol> <li>Configure the prometheus targets endpoints:</li> </ol> <p>a) Prometheus Server: edit the file roles/prometheus/templates/prometheus-config.yaml.j2</p> <pre><code>global:\n  evaluation_interval: 5s\n  external_labels:\n    env: dev\n  scrape_interval: 30s\nscrape_configs:\n- job_name: mailbox-controller\n  scrape_interval: 15s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:10203'\n\n- job_name: node-exporter\n  scrape_interval: 15s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:9100'\n\n- job_name: kcp\n  scrape_interval: 15s\n  scheme: https\n  metrics_path: /metrics\n  tls_config:\n    insecure_skip_verify: true\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:6443'\n</code></pre> <p>b) Prometheus Agent: edit the file roles/prometheus/templates/prometheus-agent-config.yaml.j2</p> <pre><code># my global config\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s \nscrape_configs:\n  - job_name: \"prometheus-agent\"\n\n    static_configs:\n      - targets: [\"{ ansible_default_ipv4.address }:&lt;port_number&gt;\"]\nremote_write:\n  - url: \"http://&lt;prometheus-server-ip-address&gt;:&lt;port_number&gt;/api/v1/write\"\n</code></pre> <ol> <li>Install prometheus server, grafana and node exporter to the KCP server host using the following playbook:</li> </ol> <pre><code>- hosts: kcp-server\n  remote_user: ubuntu\n  become: yes\n  gather_facts: yes\n  connection: ssh\n  tasks:\n  roles:\n    - node-exporter\n    - prometheus\n    - grafana\n</code></pre> <pre><code>ansible-playbook -i hosts monitoring-kcpServer.yaml\n</code></pre> <ol> <li>Install prometheus agent and node exporter to an edge pcluster using the following playbook:</li> </ol> <pre><code>- hosts: edge-pclusters\n  remote_user: ubuntu\n  become: yes\n  gather_facts: yes\n  connection: ssh\n  vars:\n   agent: 'yes'\n  tasks:\n  roles:\n    - node-exporter\n    - prometheus\n</code></pre> <pre><code>ansible-playbook -i hosts monitoring-pcluster.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/","title":"Development-Environment (dev-env)","text":"<p>under construction - coming soon</p>"},{"location":"Coding%20Milestones/PoC2023q3/outline/","title":"Coming Soon","text":""},{"location":"Community/_index/","title":"Join the KubeStellar community","text":""},{"location":"Community/_index/#kubestellar-is-an-open-source-project-that-anyone-in-the-community-can-use-improve-and-enjoy-join-us-heres-a-few-ways-to-find-out-whats-happening-and-get-involved","title":"KubeStellar is an open source project that anyone in the community can use, improve, and enjoy. Join us! Here's a few ways to find out what's happening and get involved","text":""},{"location":"Community/_index/#learn-and-connect","title":"Learn and Connect","text":""},{"location":"Community/_index/#using-or-want-to-use-kubestellar-find-out-more-here","title":"Using or want to use KubeStellar? Find out more here:","text":"<ul> <li>User mailing list: Discussion and help from your fellow users</li> <li>YouTube Channel: Follow us on YouTube to view recordings of past KubeStellar community meetings and demo days</li> <li>LinkedIn: See what others are saying about the community</li> <li>Medium Blog Series: Follow us on Medium to read about community developments</li> </ul>"},{"location":"Community/_index/#develop-and-contribute","title":"Develop and Contribute","text":""},{"location":"Community/_index/#if-you-want-to-get-more-involved-by-contributing-to-kubestellar-join-us-here","title":"If you want to get more involved by contributing to KubeStellar, join us here:","text":"<ul> <li>GitHub: Development takes place here!</li> <li>#kubestellar-dev Slack channel in the Kubernetes slack workspace: Chat with other project developers</li> <li>Developer mailing list: Discuss development issues around the project</li> <li>You can find out how to contribute to KubeStellar in our Contribution Guidelines</li> </ul>"},{"location":"Community/_index/#community-meetings","title":"Community Meetings","text":"<ol> <li>Join our Developer mailing list to get your community meeting invitation.</li> <li>You can also directly subscribe to the community calendar, or view our calendar</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Sign up to discuss a topic in the KubeStellar Community Meeting Agenda</li> </ol>"},{"location":"Community/_index/#other-resources","title":"Other Resources","text":"<ul> <li>Google Drive</li> </ul>"},{"location":"Contribution%20guidelines/CONTRIBUTING/","title":"Contributing to KubeStellar","text":"<p>Greetings! We are grateful for your interest in joining the KubeStellar community and making a positive impact. Whether you're raising issues, enhancing documentation, fixing bugs, or developing new features, your contributions are essential to our success.</p> <p>To get started, kindly read through this document and familiarize yourself with our code of conduct. If you have any inquiries, please feel free to reach out to us on Slack.</p> <p>We can't wait to collaborate with you!</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#contributing-code","title":"Contributing Code","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/#prerequisites","title":"Prerequisites","text":"<p>Install Go 1.19+.   Please note that the go language version numbers in these files must exactly agree:</p> <pre><code>Your local go/go.mod file, kcp/.ci-operator.yaml, kcp/Dockerfile, and in all the kcp/.github/workflows yaml files that specify go-version.\n\n- In ./ci-operator.yaml the go version is indicated by the \"tag\" attribute.\n- In ./Dockerfile it is indicated by the \"golang\" attribute\n- In go.mod it is indicated by the \"go\" directive.\n- In the .github/workflows yaml files it is indicated by \"go-version\"\n</code></pre> <p>Check out our Quickstart Guide</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#issues","title":"Issues","text":"<p>Prioritization for pull requests is given to those that address and resolve existing GitHub issues. Utilize the available issue labels to identify meaningful and relevant issues to work on.</p> <p>If you believe that there is a need for a fix and no existing issue covers it, feel free to create a new one.</p> <p>As a new contributor, we encourage you to start with issues labeled as good first issues.</p> <p>Your assistance in improving documentation is highly valued, regardless of your level of experience with the project.</p> <p>To claim an issue that you are interested in, kindly leave a comment on the issue and request the maintainers to assign it to you.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#committing","title":"Committing","text":"<p>We encourage all contributors to adopt best practices in git commit management to facilitate efficient reviews and retrospective analysis. Your git commits should provide ample context for reviewers and future codebase readers.</p> <p>A recommended format for final commit messages is as follows:</p> <pre><code>{Short Title}: {Problem this commit is solving and any important contextual information} {issue number if applicable}\n</code></pre>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>When submitting a pull request, clear communication is appreciated. This can be achieved by providing the following information:</p> <ul> <li>Detailed description of the problem you are trying to solve, along with links to related GitHub issues</li> <li>Explanation of your solution, including links to any design documentation and discussions</li> <li>Information on how you tested and validated your solution</li> <li>Updates to relevant documentation and examples, if applicable</li> </ul> <p>The pull request template has been designed to assist you in communicating this information effectively.</p> <p>Smaller pull requests are typically easier to review and merge than larger ones. If your pull request is big, it is always recommended to collaborate with the maintainers to find the best way to divide it.</p> <p>Approvers will review your PR within a business day. A PR requires both an /lgtm and then an /approve in order to get merged. You may /approve your own PR but you may not /lgtm it. Automation will add the PR it to the OpenShift PR merge queue. The OpenShift Tide bot will automatically merge your work when it is available.</p> <p>Congratulations! Your pull request has been successfully merged! \ud83d\udc4f</p> <p>If you have any questions about contributing, don't hesitate to reach out to us on the KCP-dev Slack channel.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-locally","title":"Testing Locally","text":"<p>Our QUICKSTART  guide shows a user how to install a local KCP server and install the KubeStellar components and run an example.  As a contributor you will want a different setup flow, including <code>git clone</code> of this repo instead of fetching and unpacking a release archive.  The same example usage should work for you, and there is a larger example at this link .</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-changes-to-the-bootstrap-script","title":"Testing changes to the bootstrap script","text":"<p>The quickstart says to fetch the bootstrap script from the main branch of the main repo; if you want to contribute a change to that script then you will need to test your changed version.  Just run your local copy (perhaps in a special testing directory, just to be safe) and be sure to add the downloaded <code>bin</code> at the front of your <code>$PATH</code> (contrary to what the scripting currently tells you) so that your <code>git clone</code>'s <code>bin</code> does not shadow the one being tested.</p> <p>Note that changes to the bootstrap script start being used by users as soon as your PR merges.  Since this script can only fetch a released version of the executables, changes to this script can not rely on any behavior of those executables that is not in the currently latest release.  Also, a change that restricts the range of usable releases needs to add checking for use of incompatible releases.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-the-bootstrap-script-against-an-upcoming-release","title":"Testing the bootstrap script against an upcoming release","text":"<p>Prior to making a new release, there needs to be testing that the current bootstrap script works with the exeutable behavior that will appear in the new release.  To support this we will add an option to the bootstrrap script that enables it to use a local release archive instead of fetching an archive of an actual release from github.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#licensing","title":"Licensing","text":"<p>KubeStellar is Apache 2.0 licensed and we accept contributions via GitHub pull requests.</p> <p>Please read the following guide if you're interested in contributing to KubeStellar.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#certificate-of-origin","title":"Certificate of Origin","text":"<p>By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.</p>"},{"location":"Contribution%20guidelines/LICENSE/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following    boilerplate notice, with the fields enclosed by brackets \"[]\"    replaced with your own identifying information. (Don't include    the brackets!)  The text should be enclosed in the appropriate    comment syntax for the file format. We also recommend that a    file or class name and description of purpose be included on the    same \"printed page\" as the copyright notice for easier    identification within third-party archives.</p> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"Contribution%20guidelines/coc/","title":"Code of Conduct","text":"<p>The Code of Conduct serves as a set of rules used by the KubeStellar community to establish a safe, respectful and inclusive environment.</p>"},{"location":"Contribution%20guidelines/coc/#kubestellar-community-code-of-conduct","title":"KubeStellar Community Code of Conduct","text":"<p>The KubeStellar Community abides by the CNCF Code of Conduct.</p> <p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the KubeStellar Code of Conduct Committee Chair Andy Anderson.</p>"},{"location":"Contribution%20guidelines/coc/#contributor-code-of-conduct","title":"Contributor Code of Conduct","text":"<p>As contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.</p> <p>We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion, or nationality.</p> <p>Examples of unacceptable behavior by participants include:</p> <p>The use of sexualized language or imagery Personal attacks Trolling or insulting/derogatory comments Public or private harassment Publishing others\u2019 private information, such as physical or electronic addresses, without explicit permission Other unethical or unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. By adopting this Code of Conduct, project maintainers commit themselves to fairly and consistently applying these principles to every aspect of managing this project. Project maintainers who do not follow or enforce the Code of Conduct may be permanently removed from the project team.</p> <p>This code of conduct applies both within project spaces and in public spaces when an individual is representing the project or its community.</p> <p>Instances of abusive, harassing, or otherwise unacceptable behavior in KubeStellar may be reported by contacting the KubeStellar Code of Conduct Committee of Conduct Committee Chair Andy Anderson. For other projects, please contact a CNCF project maintainer or our mediator, Mishi Choudhary mishi@linux.com.</p> <p>This Code of Conduct is adapted from the Contributor Covenant (http://contributor-covenant.org), version 1.2.0, available at http://contributor-covenant.org/version/1/2/0/</p> <p>CNCF Events Code of Conduct</p> <p>CNCF events are governed by the Linux Foundation Code of Conduct available on the event page. This is designed to be compatible with the above policy and also includes more details on responding to incidents.</p>"},{"location":"Getting-Started/quickstart/","title":"KubeStellar Quickstart Guide","text":""},{"location":"Getting-Started/quickstart/#estimated-time","title":"Estimated Time:","text":"<p>~3 minutes</p>"},{"location":"Getting-Started/quickstart/#required-packages","title":"Required Packages:","text":"<ul> <li>docker</li> <li>kind</li> <li>kubectl (version range expected: 1.23-1.25)</li> <li>jq</li> </ul>"},{"location":"Getting-Started/quickstart/#setup-instructions","title":"Setup Instructions","text":"<p>Table of contents:</p> <ul> <li>1. Install and run KubeStellar</li> <li>2. Example deployment of Apache HTTP Server workload into two local kind clusters</li> <li>a. Stand up two kind clusters: florin and guilder</li> <li>b. Create a KubeStellar Inventory Management Workspace (IMW) and Workload Management Workspace (WMW)</li> <li>c. Onboarding the clusters</li> <li>d. Create and deploy the Apache Server workload into florin and guilder clusters</li> <li>e. Carrying on</li> <li>3. Teardown the environment</li> </ul> <p>This guide is intended to show how to (1) quickly bring up a KubeStellar environment with its dependencies from a binary release and then (2) run through a simple example usage.</p>"},{"location":"Getting-Started/quickstart/#1-install-and-run-kubestellar","title":"1. Install and run KubeStellar","text":"<p>KubeStellar works in the context of kcp, so to use KubeStellar you also need kcp. Download the kcp and KubeStellar binaries and scripts into a <code>kubestellar</code> subfolder in your current working directory using the following command:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kcp-dev/edge-mc/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version latest\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\nexport KUBECONFIG=\"$(pwd)/.kcp/admin.kubeconfig\"\n</code></pre> <p>Check that <code>KubeStellar</code> is running:</p> <p>First, check that controllers are running with the following command:</p> <pre><code>ps aux | grep -e mailbox-controller -e placement-translator -e kubestellar-scheduler\n</code></pre> <p>which should yield something like:</p> <pre><code>user     1892  0.0  0.3 747644 29628 pts/1    Sl   10:51   0:00 mailbox-controller -v=2\nuser     1902  0.3  0.3 743652 27504 pts/1    Sl   10:51   0:02 scheduler -v 2 \nuser     1912  0.3  0.5 760428 41660 pts/1    Sl   10:51   0:02 placement-translator -v=2\n</code></pre> <p>Second, check that the Edge Service Provider Workspace (<code>espw</code>) is created with the following command:</p> <pre><code>kubectl ws tree\n</code></pre> <p>which should yield:</p> <pre><code>kubectl ws tree\n.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 compute\n    \u2514\u2500\u2500 espw\n</code></pre>"},{"location":"Getting-Started/quickstart/#2-example-deployment-of-apache-http-server-workload-into-two-local-kind-clusters","title":"2. Example deployment of Apache HTTP Server workload into two local kind clusters","text":"<p>In this example you will create two edge clusters and define one workload that will be distributed from the center to those edge clusters.  This example is similar to the one described more expansively on the website, but with the some steps reorganized and combined and the special workload and summarization aspirations removed.</p>"},{"location":"Getting-Started/quickstart/#a-stand-up-two-kind-clusters-florin-and-guilder","title":"a. Stand up two kind clusters: florin and guilder","text":"<p>Create the first edge cluster:</p> <pre><code>kind create cluster --name florin --config  kubestellar/examples/florin-config.yaml\n</code></pre> <p>Note: if you already have a cluster named 'florin' from a previous exercise of KubeStellar, please delete the florin cluster ('kind delete cluster --name florin') and create it using the instruction above.</p> <p>Create the second edge cluster:</p> <pre><code>kind create cluster --name guilder --config  kubestellar/examples/guilder-config.yaml\n</code></pre> <p>Note: if you already have a cluster named 'guilder' from a previous exercise of KubeStellar, please delete the guilder cluster ('kind delete cluster --name guilder') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart/#b-create-a-kubestellar-inventory-management-workspace-imw-and-workload-management-workspace-wmw","title":"b. Create a KubeStellar Inventory Management Workspace (IMW) and Workload Management Workspace (WMW)","text":"<p>IMWs are used by KubeStellar to store inventory objects (<code>SyncTargets</code> and <code>Locations</code>). Create an IMW named <code>example-imw</code> with the following command:</p> <pre><code>kubectl config use-context root\nkubectl ws root\nkubectl ws create example-imw\n</code></pre> <p>WMWs are used by KubeStellar to store workload descriptions and <code>EdgePlacement</code> objects. Create an WMW named <code>example-wmw</code> in a <code>my-org</code> workspace with the following commands:</p> <pre><code>kubectl ws root\nkubectl ws create my-org --enter\nkubectl kubestellar ensure wmw example-wmw\n</code></pre> <p>A WMW does not have to be created before the edge cluster is on-boarded; the WMW only needs to be created before content is put in it.</p>"},{"location":"Getting-Started/quickstart/#c-onboarding-the-clusters","title":"c. Onboarding the clusters","text":"<p>Let's begin by onboarding the <code>florin</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:example-imw florin env=prod\n</code></pre> <p>which should yield something like:</p> <pre><code>Current workspace is \"root:example-imw\".\nsynctarget.workload.kcp.io/florin created\nlocation.scheduling.kcp.io/florin created\nsynctarget.workload.kcp.io/florin labeled\nlocation.scheduling.kcp.io/florin labeled\nCurrent workspace is \"root:example-imw\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:9nemli4rpx83ahnz-mb-c44d04db-ae85-422c-9e12-c5e7865bf37a\" (type root:universal).\nCreating service account \"kcp-edge-syncer-florin-1yi5q9c4\"\nCreating cluster role \"kcp-edge-syncer-florin-1yi5q9c4\" to give service account \"kcp-edge-syncer-florin-1yi5q9c4\"\n 1. write and sync access to the synctarget \"kcp-edge-syncer-florin-1yi5q9c4\"\n 2. write access to apiresourceimports.\nCreating or updating cluster role binding \"kcp-edge-syncer-florin-1yi5q9c4\" to bind service account \"kcp-edge-syncer-florin-1yi5q9c4\" to cluster role \"kcp-edge-syncer-florin-1yi5q9c4\".\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kcp-edge-syncer-florin-1yi5q9c4\". Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n  KUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kcp-edge-syncer-florin-1yi5q9c4\" kcp-edge-syncer-florin-1yi5q9c4\nto verify the syncer pod is running.\nCurrent workspace is \"root:example-imw\".\nCurrent workspace is \"root\".\n</code></pre> <p>An edge syncer manifest yaml file was created in your current director: <code>florin-syncer.yaml</code>. The default for the output file is the name of the SyncTarget object with \u201c-syncer.yaml\u201d appended.</p> <p>Now let's deploy the edge syncer to the <code>florin</code> edge cluster:</p> <pre><code>kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <p>which should yield something like:</p> <pre><code>namespace/kcp-edge-syncer-florin-1yi5q9c4 created\nserviceaccount/kcp-edge-syncer-florin-1yi5q9c4 created\nsecret/kcp-edge-syncer-florin-1yi5q9c4-token created\nclusterrole.rbac.authorization.k8s.io/kcp-edge-syncer-florin-1yi5q9c4 created\nclusterrolebinding.rbac.authorization.k8s.io/kcp-edge-syncer-florin-1yi5q9c4 created\nsecret/kcp-edge-syncer-florin-1yi5q9c4 created\ndeployment.apps/kcp-edge-syncer-florin-1yi5q9c4 created\n</code></pre> <p>Optionally, check that the edge syncer pod is running:</p> <pre><code>kubectl --context kind-florin get pods -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                         NAME                                               READY   STATUS    RESTARTS   AGE\nkcp-edge-syncer-florin-1yi5q9c4   kcp-edge-syncer-florin-1yi5q9c4-77cb588c89-xc5qr   1/1     Running   0          12m\nkube-system                       coredns-565d847f94-92f4k                           1/1     Running   0          58m\nkube-system                       coredns-565d847f94-kgddm                           1/1     Running   0          58m\nkube-system                       etcd-florin-control-plane                          1/1     Running   0          58m\nkube-system                       kindnet-p8vkv                                      1/1     Running   0          58m\nkube-system                       kube-apiserver-florin-control-plane                1/1     Running   0          58m\nkube-system                       kube-controller-manager-florin-control-plane       1/1     Running   0          58m\nkube-system                       kube-proxy-jmxsg                                   1/1     Running   0          58m\nkube-system                       kube-scheduler-florin-control-plane                1/1     Running   0          58m\nlocal-path-storage                local-path-provisioner-684f458cdd-kw2xz            1/1     Running   0          58m\n</code></pre> <p>Now, let's onboard the <code>guilder</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:example-imw guilder env=prod extended=si\n</code></pre> <p>Apply the created edge syncer manifest:</p> <pre><code>kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre>"},{"location":"Getting-Started/quickstart/#d-create-and-deploy-the-apache-server-workload-into-florin-and-guilder-clusters","title":"d. Create and deploy the Apache Server workload into florin and guilder clusters","text":"<p>Create the <code>EdgePlacement</code> object for your workload. Its \u201cwhere predicate\u201d (the locationSelectors array) has one label selector that matches the Location objects (<code>florin</code> and <code>guilder</code>) created earlier, thus directing the workload to both edge clusters.</p> <p>In the <code>example-wmw</code> workspace create the following <code>EdgePlacement</code> object: </p> <pre><code>kubectl ws root:my-org:example-wmw\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  namespaceSelector:\n    matchLabels: {\"common\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <p>Put the prescription of the HTTP server workload into the WMW. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>edge-placement-c</code>) object created above. </p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Now, let's check that the deployment was created in the <code>florin</code> edge cluster - it may take a few 10s of seconds to appear:</p> <pre><code>kubectl --context kind-florin get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                         NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                       commond                           1/1     1            1           6m48s\nkcp-edge-syncer-florin-2upj1awn   kcp-edge-syncer-florin-2upj1awn   1/1     1            1           16m\nkube-system                       coredns                           2/2     2            2           28m\nlocal-path-storage                local-path-provisioner            1/1     1            1           28m\n</code></pre> <p>Also, let's check that the deployment was created in the <code>guilder</code> edge cluster:</p> <pre><code>kubectl --context kind-guilder get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                        commond                            1/1     1            1           7m54s\nkcp-edge-syncer-guilder-6tuay5d6   kcp-edge-syncer-guilder-6tuay5d6   1/1     1            1           12m\nkube-system                        coredns                            2/2     2            2           27m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           27m\n</code></pre> <p>Lastly, let's check that the workload is working in both clusters:</p> <p>For <code>florin</code>:</p> <p><pre><code>curl http://localhost:8081\n</code></pre> which should yield:</p> <p><pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> NOTE: if you receive the error: 'curl: (52) Empty reply from server', wait 2 minutes and attempt curl again.  It takes a minute for the Apache HTTP Server to synchronize and start.</p> <p>For <code>guilder</code>:</p> <p><pre><code>curl http://localhost:8096\n</code></pre> which should yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>NOTE: if you receive the error: 'curl: (52) Empty reply from server', wait and attempt curl again.  It takes some time for the Apache HTTP Server to synchronize and start.</p> <p>Congratulations, you\u2019ve just deployed a workload to two edge clusters using kubestellar! To learn more about kubestellar please visit our User Guide</p>"},{"location":"Getting-Started/quickstart/#e-carrying-on","title":"e. Carrying on","text":"<p>What you just did is part of the example on the website, but with the some steps reorganized and combined and the special workload and summarization aspiration removed.  You could continue from here, doing the steps for the special workload.</p>"},{"location":"Getting-Started/quickstart/#3-teardown-the-environment","title":"3. Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl ws root:my-org\nkubectl kubestellar remove wmw demo1\nkubectl ws root\nkubectl delete workspace my-org\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Stop and uninstall KubeStellar use the following command:</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Getting-Started/quickstart/#demo-video","title":"Demo Video","text":""},{"location":"Getting-Started/user-guide/","title":"User Guide","text":"<p>Coming Soon</p>"}]}